{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "translation_MHA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOXxy2nkJMSInR0DHgFGl2J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/malinphy/Embedding_calls/blob/main/translation_MHA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWWODIgAlZjj",
        "outputId": "7eb9529a-2ca9-4a7c-eb1c-d70227d1d9da"
      },
      "source": [
        "#@In this note I will try to use tfa MultuHeadAttention layer \n",
        "\n",
        "!pip install -q tensorflow_datasets\n",
        "!pip install -q -U tensorflow-text\n",
        "!pip install tensorflow-addons\n",
        "!pip install tensorflow-text\n",
        "import tensorflow as  tf\n",
        "\n",
        "\n",
        "import collections\n",
        "import logging\n",
        "import os\n",
        "import pathlib\n",
        "import re\n",
        "import string\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_text as text\n",
        "\n",
        "from tensorflow_addons.layers import MultiHeadAttention\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow_addons import layers\n",
        "\n",
        "logging.getLogger('tensorflow').setLevel(logging.ERROR)  # suppress warnings"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.7/dist-packages (0.13.0)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.7/dist-packages (2.5.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text) (0.12.0)\n",
            "Requirement already satisfied: tensorflow<2.6,>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text) (2.5.0)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text) (2.5.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text) (1.12)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text) (0.2.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text) (3.17.3)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text) (1.1.2)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text) (1.12.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text) (2.5.0)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text) (3.1.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text) (0.36.2)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text) (3.7.4.3)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text) (1.34.1)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text) (1.6.3)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text) (0.12.0)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text) (0.4.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text) (1.19.5)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text) (3.3.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text) (1.1.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow<2.6,>=2.5.0->tensorflow-text) (1.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text) (0.4.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text) (1.8.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text) (1.32.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text) (57.2.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text) (1.0.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text) (4.2.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text) (4.6.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text) (3.5.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZBbvW7zmk7n"
      },
      "source": [
        "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True,\n",
        "                               as_supervised=True)\n",
        "train_examples, val_examples = examples['train'], examples['validation']"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ti-kWiZxmxkT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e97da41b-e9c7-4cc4-cda6-2882064c8855"
      },
      "source": [
        "for pt_examples, en_examples in train_examples.batch(3).take(1):\n",
        "  for pt in pt_examples.numpy():\n",
        "    print(pt.decode('utf-8'))\n",
        "\n",
        "  print()\n",
        "\n",
        "  for en in en_examples.numpy():\n",
        "    print(en.decode('utf-8'))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "e quando melhoramos a procura , tiramos a única vantagem da impressão , que é a serendipidade .\n",
            "mas e se estes fatores fossem ativos ?\n",
            "mas eles não tinham a curiosidade de me testar .\n",
            "\n",
            "and when you improve searchability , you actually take away the one advantage of print , which is serendipity .\n",
            "but what if it were active ?\n",
            "but they did n't test for curiosity .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dh0pRsPImzeE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "3a217755-8a89-4faa-f6ee-7b7e08185404"
      },
      "source": [
        "model_name = \"ted_hrlr_translate_pt_en_converter\"\n",
        "tf.keras.utils.get_file(\n",
        "    f\"{model_name}.zip\",\n",
        "    f\"https://storage.googleapis.com/download.tensorflow.org/models/{model_name}.zip\",\n",
        "    cache_dir='.', cache_subdir='', extract=True\n",
        ")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'./ted_hrlr_translate_pt_en_converter.zip'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "029O1KlM0LiO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "329772e7-059d-4c2e-8177-43602479e642"
      },
      "source": [
        "tokenizers = tf.saved_model.load(model_name)\n",
        "[item for item in dir(tokenizers.en) if not item.startswith('_')]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['detokenize',\n",
              " 'get_reserved_tokens',\n",
              " 'get_vocab_path',\n",
              " 'get_vocab_size',\n",
              " 'lookup',\n",
              " 'tokenize',\n",
              " 'tokenizer',\n",
              " 'vocab']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsL4F6M50NF8"
      },
      "source": [
        ""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQG3nb0I0OtF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec485c72-6150-44c4-89a4-f5306d631751"
      },
      "source": [
        "for en in en_examples.numpy():\n",
        "  print(en.decode('utf-8'))\n",
        "\n",
        "encoded = tokenizers.en.tokenize(en_examples)\n",
        "\n",
        "for row in encoded.to_list():\n",
        "  print(row)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "and when you improve searchability , you actually take away the one advantage of print , which is serendipity .\n",
            "but what if it were active ?\n",
            "but they did n't test for curiosity .\n",
            "[2, 72, 117, 79, 1259, 1491, 2362, 13, 79, 150, 184, 311, 71, 103, 2308, 74, 2679, 13, 148, 80, 55, 4840, 1434, 2423, 540, 15, 3]\n",
            "[2, 87, 90, 107, 76, 129, 1852, 30, 3]\n",
            "[2, 87, 83, 149, 50, 9, 56, 664, 85, 2512, 15, 3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uLpqFHJ0RVw"
      },
      "source": [
        ""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2lo9Vwa0TiU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "379a220f-82ff-4fd3-8bb2-a1de931e7b22"
      },
      "source": [
        "round_trip = tokenizers.en.detokenize(encoded)\n",
        "for line in round_trip.numpy():\n",
        "  print(line.decode('utf-8'))\n",
        "\n",
        "tokens = tokenizers.en.lookup(encoded)\n",
        "tokens"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "and when you improve searchability , you actually take away the one advantage of print , which is serendipity .\n",
            "but what if it were active ?\n",
            "but they did n ' t test for curiosity .\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'[START]', b'and', b'when', b'you', b'improve', b'search', b'##ability', b',', b'you', b'actually', b'take', b'away', b'the', b'one', b'advantage', b'of', b'print', b',', b'which', b'is', b's', b'##ere', b'##nd', b'##ip', b'##ity', b'.', b'[END]'], [b'[START]', b'but', b'what', b'if', b'it', b'were', b'active', b'?', b'[END]'], [b'[START]', b'but', b'they', b'did', b'n', b\"'\", b't', b'test', b'for', b'curiosity', b'.', b'[END]']]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sH18DVT-0VT1"
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVf_H5GL0XBt"
      },
      "source": [
        "def tokenize_pairs(pt, en):\n",
        "    pt = tokenizers.pt.tokenize(pt)\n",
        "    # Convert from ragged to dense, padding with zeros.\n",
        "    pt = pt.to_tensor()\n",
        "\n",
        "    en = tokenizers.en.tokenize(en)\n",
        "    # Convert from ragged to dense, padding with zeros.\n",
        "    en = en.to_tensor()\n",
        "    return pt, en"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MxTINUs0Yfc"
      },
      "source": [
        "BUFFER_SIZE = 20000\n",
        "BATCH_SIZE = 64"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHPZXOcqL8k8"
      },
      "source": [
        "# def scaled_dot_product_attention(q, k, v, mask):\n",
        "#   \"\"\"Calculate the attention weights.\n",
        "#   q, k, v must have matching leading dimensions.\n",
        "#   k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "#   The mask has different shapes depending on its type(padding or look ahead)\n",
        "#   but it must be broadcastable for addition.\n",
        "\n",
        "#   Args:\n",
        "#     q: query shape == (..., seq_len_q, depth)\n",
        "#     k: key shape == (..., seq_len_k, depth)\n",
        "#     v: value shape == (..., seq_len_v, depth_v)\n",
        "#     mask: Float tensor with shape broadcastable\n",
        "#           to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "\n",
        "#   Returns:\n",
        "#     output, attention_weights\n",
        "#   \"\"\"\n",
        "\n",
        "#   matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "#   # scale matmul_qk\n",
        "#   dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "#   scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "#   # add the mask to the scaled tensor.\n",
        "#   if mask is not None:\n",
        "#     scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "#   # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "#   # add up to 1.\n",
        "#   attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "#   output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "#   return output, attention_weights\n",
        "\n",
        "# class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "#   def __init__(self, d_model, num_heads):\n",
        "#     super(MultiHeadAttention, self).__init__()\n",
        "#     self.num_heads = num_heads\n",
        "#     self.d_model = d_model\n",
        "\n",
        "#     assert d_model % self.num_heads == 0\n",
        "\n",
        "#     self.depth = d_model // self.num_heads\n",
        "\n",
        "#     self.wq = tf.keras.layers.Dense(d_model)\n",
        "#     self.wk = tf.keras.layers.Dense(d_model)\n",
        "#     self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "#     self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "#   def split_heads(self, x, batch_size):\n",
        "#     \"\"\"Split the last dimension into (num_heads, depth).\n",
        "#     Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "#     \"\"\"\n",
        "#     x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "#     return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "#   def call(self, v, k, q, mask):\n",
        "#     batch_size = tf.shape(q)[0]\n",
        "\n",
        "#     q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "#     k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "#     v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "#     q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "#     k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "#     v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "\n",
        "#     # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "#     # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "#     scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "#         q, k, v, mask)\n",
        "\n",
        "#     scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "#     concat_attention = tf.reshape(scaled_attention,\n",
        "#                                   (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "#     output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "#     return output, attention_weights\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tz4o6wg20aM8"
      },
      "source": [
        "def make_batches(ds):\n",
        "  return (\n",
        "      ds\n",
        "      .cache()\n",
        "      .shuffle(BUFFER_SIZE)\n",
        "      .batch(BATCH_SIZE)\n",
        "      .map(tokenize_pairs, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "      .prefetch(tf.data.AUTOTUNE))\n",
        "\n",
        "\n",
        "train_batches = make_batches(train_examples)\n",
        "val_batches = make_batches(val_examples)\n",
        "\n",
        "def get_angles(pos, i, d_model):\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "  return pos * angle_rates\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "\n",
        "  # apply sin to even indices in the array; 2i\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "  # apply cos to odd indices in the array; 2i+1\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRFVF9qi0bLE"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtd8YOHr0clg"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvPkZZOu0dzx"
      },
      "source": [
        "def create_padding_mask(seq):\n",
        "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "\n",
        "  # add extra dimensions to add the padding\n",
        "  # to the attention logits.\n",
        "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
        "\n",
        "\n",
        "def create_look_ahead_mask(size):\n",
        "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "  return mask  # (seq_len, seq_len)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7_LYVgq0ia_"
      },
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "  ])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNxFfHu9UdnD",
        "outputId": "43fc6f29-f578-43fd-f873-141d8492a87e"
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "\n",
        "    self.mha = MultiHeadAttention(d_model, num_heads, return_attn_coef=True)\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "    if mask is None:\n",
        "      attn_output, _ = self.mha([x, x, x])\n",
        "\n",
        "    if mask is not None:\n",
        "\n",
        "      attn_output, _ = self.mha([x, x, x], mask)  # (batch_size, input_seq_len, d_model)\n",
        "    attn_output = self.dropout1(attn_output, training=training)\n",
        "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\n",
        "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    return out2\n",
        "\n",
        "\n",
        "sample_encoder_layer = EncoderLayer(512, 8, 2048)\n",
        "\n",
        "sample_encoder_layer_output = sample_encoder_layer(\n",
        "    tf.random.uniform((64, 43, 512)), False, None)\n",
        "\n",
        "sample_encoder_layer_output.shape  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 43, 512])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqBw1z0FWX6r",
        "outputId": "f080ed7a-02ea-447b-b6ea-fb104abed36a"
      },
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.mha1 = MultiHeadAttention(d_model, num_heads, return_attn_coef=True)\n",
        "    self.mha2 = MultiHeadAttention(d_model, num_heads, return_attn_coef=True)\n",
        "\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, enc_output, training,\n",
        "           look_ahead_mask, padding_mask):\n",
        "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    if look_ahead_mask is None:    \n",
        "      attn1, attn_weights_block1 = self.mha1([x, x, x])  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    if look_ahead_mask is not None:\n",
        "      attn1, attn_weights_block1 = self.mha1([x, x, x], look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    attn1 = self.dropout1(attn1, training=training)\n",
        "    out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "\n",
        "    if padding_mask is None:    \n",
        "      attn2, attn_weights_block2 = self.mha2([enc_output, enc_output, out1],)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    if padding_mask is not None:\n",
        "      attn2, attn_weights_block2 = self.mha2([enc_output, enc_output, out1], padding_mask)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    attn2 = self.dropout2(attn2, training=training)\n",
        "    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
        "    ffn_output = self.dropout3(ffn_output, training=training)\n",
        "    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    return out3, attn_weights_block1, attn_weights_block2\n",
        "\n",
        "\n",
        "sample_decoder_layer = DecoderLayer(512, 8, 2048)\n",
        "\n",
        "sample_decoder_layer_output, _, _ = sample_decoder_layer(\n",
        "    tf.random.uniform((64, 43, 512)), sample_encoder_layer_output,\n",
        "    False, None, None)\n",
        "\n",
        "sample_decoder_layer_output.shape  # (batch_size, target_seq_len, d_model)\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 43, 512])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-ijfXaJ0uj8"
      },
      "source": [
        ""
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBMTUpFD0wrm"
      },
      "source": [
        ""
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kxa5LJ8vRNjB"
      },
      "source": [
        ""
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1oqwyopYvuE",
        "outputId": "b982660c-88e7-43e1-81d1-d9f5b3953906"
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding,\n",
        "                                            self.d_model)\n",
        "\n",
        "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate)\n",
        "                       for _ in range(num_layers)]\n",
        "\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "\n",
        "    # adding embedding and position encoding.\n",
        "    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x, training, mask)\n",
        "\n",
        "    return x  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "sample_encoder = Encoder(num_layers=2, d_model=512, num_heads=8,\n",
        "                         dff=2048, input_vocab_size=8500,\n",
        "                         maximum_position_encoding=10000)\n",
        "temp_input = tf.random.uniform((64, 62), dtype=tf.int64, minval=0, maxval=200)\n",
        "\n",
        "sample_encoder_output = sample_encoder(temp_input, training=False, mask=None)\n",
        "\n",
        "print(sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 62, 512)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSGSHokfYvpn",
        "outputId": "aaf16ab5-2bd9-4369-938d-f9ac0143d077"
      },
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate)\n",
        "                       for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, enc_output, training,\n",
        "           look_ahead_mask, padding_mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    attention_weights = {}\n",
        "\n",
        "    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
        "                                             look_ahead_mask, padding_mask)\n",
        "\n",
        "      attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
        "      attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
        "\n",
        "    # x.shape == (batch_size, target_seq_len, d_model)\n",
        "    return x, attention_weights\n",
        "\n",
        "\n",
        "sample_decoder = Decoder(num_layers=2, d_model=512, num_heads=8,\n",
        "                         dff=2048, target_vocab_size=8000,\n",
        "                         maximum_position_encoding=5000)\n",
        "temp_input = tf.random.uniform((64, 62), dtype=tf.int64, minval=0, maxval=200)\n",
        "\n",
        "output, attn = sample_decoder(temp_input,\n",
        "                              enc_output=sample_encoder_output,\n",
        "                              training=False,\n",
        "                              look_ahead_mask=None,\n",
        "                              padding_mask=None)\n",
        "\n",
        "output.shape, attn['decoder_layer2_block2'].shape\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 62, 512]), TensorShape([64, 8, 62, 62]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lw3HvdjNYvlf",
        "outputId": "7a3f675c-bd01-46a0-b7d6-134e02f5a95f"
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "    super(Transformer, self).__init__()\n",
        "\n",
        "    self.encoder = Encoder(num_layers, d_model, num_heads, dff,\n",
        "                             input_vocab_size, pe_input, rate)\n",
        "\n",
        "    self.decoder = Decoder(num_layers, d_model, num_heads, dff,\n",
        "                           target_vocab_size, pe_target, rate)\n",
        "\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "  def call(self, inp, tar, training, enc_padding_mask,\n",
        "           look_ahead_mask, dec_padding_mask):\n",
        "\n",
        "    enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
        "\n",
        "    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "    dec_output, attention_weights = self.decoder(\n",
        "        tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "\n",
        "    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "\n",
        "    return final_output, attention_weights\n",
        "\n",
        "sample_transformer = Transformer(\n",
        "    num_layers=2, d_model=512, num_heads=8, dff=2048,\n",
        "    input_vocab_size=8500, target_vocab_size=8000,\n",
        "    pe_input=10000, pe_target=6000)\n",
        "\n",
        "temp_input = tf.random.uniform((64, 62), dtype=tf.int64, minval=0, maxval=200)\n",
        "temp_target = tf.random.uniform((64, 62), dtype=tf.int64, minval=0, maxval=200)\n",
        "\n",
        "fn_out, _ = sample_transformer(temp_input, temp_target, training=False,\n",
        "                               enc_padding_mask=None,\n",
        "                               look_ahead_mask=None,\n",
        "                               dec_padding_mask=None)\n",
        "\n",
        "fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 62, 8000])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0tk6_10c7x1"
      },
      "source": [
        "num_layers = 4\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "dropout_rate = 0.1\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K77bXvElc7uK"
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PI5DQcZIc7qL"
      },
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
        "                                     epsilon=1e-9)\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuFCQLbFc7kP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "e0a81792-aec7-4744-cc8b-9aab26895b1f"
      },
      "source": [
        "temp_learning_rate_schedule = CustomSchedule(d_model)\n",
        "\n",
        "plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.xlabel(\"Train Step\")\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Train Step')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEGCAYAAACtqQjWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxV9Zn48c+TfYEEsrAGCIQABotbpGjdqYK2Shdasc6vdqT113EZp3Zq9dcZx3HqTLXTYm21DipuoyLFLthaUeu+sAQXZBFIbhDCehMgkACBJM/vj/NNuISb5Ca5N/cm93m/Xnnl3O8553ueewN5cs73e54jqooxxhgTDgnRDsAYY0z/YUnFGGNM2FhSMcYYEzaWVIwxxoSNJRVjjDFhkxTtAKIpLy9PCwsLox2GMcb0KatWrapW1fxg6+I6qRQWFlJWVhbtMIwxpk8Rkc/aW2eXv4wxxoSNJRVjjDFhY0nFGGNM2FhSMcYYEzaWVIwxxoRNRJOKiMwUkQ0iUi4itwVZnyoiz7n1y0WkMGDd7a59g4jMCGhfICK7RWRNO8f8oYioiORF4j0ZY4xpX8SSiogkAg8AlwIlwFUiUtJms7nAXlUdD8wD7nH7lgBzgMnATOBB1x/A464t2DFHAZcAW8L6ZowxxoQkkmcqU4FyVfWp6hFgITCrzTazgCfc8mJguoiIa1+oqg2qWgmUu/5Q1beAPe0ccx5wK9Av6/mrKotWbqWuoTHaoRhjTFCRTCojga0Br6tcW9BtVLURqAVyQ9z3OCIyC9imqh93st11IlImImV+vz+U9xEzPtq6j1ufX82PF6+OdijGGBNUvxioF5EM4P8Bd3S2rarOV9VSVS3Nzw9aZSBmbdlzEIBX1u+KciTGGBNcJJPKNmBUwOsC1xZ0GxFJArKBmhD3DVQEjAU+FpHNbvsPRGRYD+KPORX+egCONDaz1SUYY4yJJZFMKiuBYhEZKyIpeAPvS9psswS4xi3PBl5T7/nGS4A5bnbYWKAYWNHegVT1E1UdoqqFqlqId7nsdFXdGd63FF0V/jpEvOW/rtkR3WCMMSaIiCUVN0ZyI7AUWA8sUtW1InKXiFzhNnsUyBWRcuAW4Da371pgEbAOeAm4QVWbAETkWeB9YKKIVInI3Ei9h1jj89dz/oR8Jo/I4q9r+lW+NMb0ExGtUqyqLwIvtmm7I2D5MPCNdva9G7g7SPtVIRy3sKuxxrrmZqWyuo6zi3I5szCHny/dwI7aQwzPTo92aMYY06pfDNTHg+21hzh8tJlx+ZlcerI3VPSSna0YY2KMJZU+wucG6YvyBzAufwCThg3kz6ttXMUYE1ssqfQRFf46AMblZwIw69SRrPpsL5/V1EczLGOMOY4llT7C569nYFoS+QNSAZh16ghE4I8fbo9yZMYYc4wllT6iwl/HuPwBiJtTPGJQOtPG5vKHD6vwZmEbY0z0WVLpI3z+eoryMo9r++rpI9lcc5APt+6LUlTGGHM8Syp9QF1DIzv3H6ZoyIDj2i89eRipSQn84YOOig0YY0zvsaTSB1S6mV/j2pypDExL5uKSobywejsNjU3RCM0YY45jSaUP8FV7M7/anqkAfKN0FPsOHuXltVZk0hgTfZZU+oCK3XUkCIzJzThh3bnj8ygYnM4zy+25ZMaY6LOk0gdUVNdTMDiD1KTEE9YlJAhXTR3N+74afO5eFmOMiRZLKn1Axe46ivIz213/jdICkhKEhSu3truNMcb0BksqMa65WdlcU8+4/BPHU1oMGZjGxSVDWbyqygbsjTFRZUklxrUUkizqIKkAXDV1NHvqj1iRSWNMVFlSiXEtT3sc18HlL4BzxucxNi+TBe9utjvsjTFRY0klxrUMvnd2ppKQIFz7hUI+3rqPD7bs7Y3QjDHmBJZUYlyFv46BaUnkDUjpdNuvn1FAdnoyj7xd2QuRGWPMiSypxDifv/64QpIdyUhJ4qqpo1m6didb9xzsheiMMeZ4llRinM9f3+F04rauOXsMCSI8/t7myAVljDHtiGhSEZGZIrJBRMpF5LYg61NF5Dm3frmIFAasu921bxCRGQHtC0Rkt4isadPXz0XkUxFZLSJ/EJFBkXxvvaG1kGQn4ymBhmen86Upw3lu5VZqDx6NYHTGGHOiiCUVEUkEHgAuBUqAq0SkpM1mc4G9qjoemAfc4/YtAeYAk4GZwIOuP4DHXVtbrwAnq+oUYCNwe1jfUBRUtj5COPQzFYDvn19EXUMjj71nYyvGmN4VyTOVqUC5qvpU9QiwEJjVZptZwBNueTEwXbzBg1nAQlVtUNVKoNz1h6q+BexpezBVfVlVG93LZUBBuN9Qbzv2COHQz1QAThqexcUlQ1nwTiUHDtvZijGm90QyqYwEAuuGVLm2oNu4hFAL5Ia4b0euBf4abIWIXCciZSJS5vf7u9Bl7/P52y8k2ZmbLhrP/sONPLXsswhEZowxwfW7gXoR+QnQCDwdbL2qzlfVUlUtzc/P793guqjCX8+onOCFJDszpWAQ50/I55G3Kzl4pLHzHYwxJgwimVS2AaMCXhe4tqDbiEgSkA3UhLjvCUTkO8CXgau1H9xWXuGvO+HBXF3xj9PHs6f+CE8vs7L4xpjeEcmkshIoFpGxIpKCN/C+pM02S4Br3PJs4DWXDJYAc9zssLFAMbCio4OJyEzgVuAKVe3zN2k0NyuV1fVdmvnV1hljcji3OI8H3yhnv42tGGN6QcSSihsjuRFYCqwHFqnqWhG5S0SucJs9CuSKSDlwC3Cb23ctsAhYB7wE3KCqTQAi8izwPjBRRKpEZK7r6zfAQOAVEflIRB6K1HvrDdv2HaKhsbnLg/Rt/XjmJPYePMrDb/nCFJkxxrQvKZKdq+qLwItt2u4IWD4MfKOdfe8G7g7SflU724/vUbAxxlfdvenEbZ08MpsvTxnOI29X8n/OGsOQgWnhCM8YY4LqdwP1/UXF7u5NJw7mh5dM5GhTM795rbzHfRljTEcsqcQoX3XohSQ7MzYvkyvPHMUzy7ew2Z0BGWNMJFhSiVFeza/QCkmG4ubpxaQmJfDTv6wPS3/GGBOMJZUYVeGv6/TBXF0xJCuNm6YX8+r6XbyxYXfY+jXGmECWVGJQXUMju/Y39Gg6cTB//4VCxuZlctcL6zjS2BzWvo0xBiypxKRjT3sM35kKQGpSIndcXoKvup7HrdikMSYCLKnEIF/rc+nDe6YCcOHEIUyfNIRfvbqJnbWHw96/MSa+WVKJQRU9KCQZijsuL6FJlX/90xr6QTUbY0wMsaQSg3w9KCQZijG5mfzgixN4Zd0u/rpmZ0SOYYyJT5ZUYlCFvy7sg/RtzT1nLCePzOKOP621J0QaY8LGkkqMaSkk2ZPqxKFISkzgnq9PYe/BI9z94rqIHssYEz8sqcSYlkKSRUMie6YCMHlENtedN45FZVW8bveuGGPCwJJKjGl9hHCEz1Ra3Dy9mIlDB3Lr4tXU1DX0yjGNMf2XJZUYE8npxMGkJSdy35xTqT14lNt//4nNBjPG9IgllRjjq64jK0yFJEN10vAsbp05kZfX7WJR2dZeO64xpv+xpBJjKnbXMy6MhSRDde0XxnJ2US7//sK61jv6jTGmqyypxBhfdeSnEweTkCD84punkJqUwPVPf8ChI029HoMxpu+zpBJDDhw+yq79DWGtTtwVw7PTmXflqWzYdYB/+aPdbW+M6TpLKjGkMkyPEO6JCyYO4aaLinn+gyqeW2njK8aYroloUhGRmSKyQUTKReS2IOtTReQ5t365iBQGrLvdtW8QkRkB7QtEZLeIrGnTV46IvCIim9z3wZF8b5FQ0VqduPcvfwW6eXox5xbncceStazZVhvVWIwxfUvEkoqIJAIPAJcCJcBVIlLSZrO5wF5VHQ/MA+5x+5YAc4DJwEzgQdcfwOOura3bgL+pajHwN/e6T/H560kQGB2hQpKhSkwQfjXnNPIyU7juyTJ2H7BqxsaY0ETyTGUqUK6qPlU9AiwEZrXZZhbwhFteDEwXb9rTLGChqjaoaiVQ7vpDVd8C9gQ5XmBfTwBfCeeb6Q0+fz2jI1hIsityMlOY/+1S9h48ynVPruLwURu4N8Z0LpJJZSQQeFG+yrUF3UZVG4FaIDfEfdsaqqo73PJOYGiwjUTkOhEpE5Eyv98fyvvoNd4jhKN76SvQySOzmXflqXy0dR+3Ll5tA/fGmE71y4F69X77Bf0NqKrzVbVUVUvz8/N7ObL2NblCktEcpA9m5snD+NGMiSz5eDv3/6082uEYY2JcJJPKNmBUwOsC1xZ0GxFJArKBmhD3bWuXiAx3fQ0H+lSFxO2ukGQsnam0uP6CIr52+kjmvbqRRTYjzBjTgUgmlZVAsYiMFZEUvIH3JW22WQJc45ZnA6+5s4wlwBw3O2wsUAys6OR4gX1dA/wpDO+h1/R2IcmuEBF+9rUpnFucx22/X80r63ZFOyRjTIyKWFJxYyQ3AkuB9cAiVV0rIneJyBVus0eBXBEpB27BzdhS1bXAImAd8BJwg6o2AYjIs8D7wEQRqRKRua6vnwEXi8gm4IvudZ/RUkiyN0red0dKUgIP/d0ZfK5gEDc+8wErKoPNlTDGxDuJ58HX0tJSLSsri3YYAPzkD5/wwsfb+fjfLun1ul9dsaf+CLMfeg//gQaeu+4sSkZkRTskY0wvE5FVqloabF2/HKjvi3z+eoqG9H4hya7KyUzhqbmfZ0BqElc/soxPd+6PdkjGmBhiSSVGVPjrGJcXm5e+2ho5KJ1nvzeN1KREvvXwcjbsPBDtkIwxMcKSSgw4cPgouw9Er5BkdxTmZfLsddNIShC+9fAyNu2yxGKMsaQSE1oH6WNwOnFHxrrEkpAgXPXwMtbvsEthxsQ7SyoxwFfdUkiy75yptCjKH8Cz35tGUkICV/7P+6z6zGaFGRPPOk0qIjJBRP7WUhVYRKaIyL9EPrT44fPXk5ggUS8k2V3jhwxg8T+cRe6AVP7ukRW8uTG2yt8YY3pPKGcqDwO3A0cBVHU13o2MJkwq/HWMGpweE4Uku6tgcAaL/u9ZjM3L5LtPrOTPq7dHOyRjTBSEklQyVLXt3eyNkQgmXvn89X1uPCWY/IGpLPy/0zht1GBuevZD5r9VYUUojYkzoSSVahEpwhVoFJHZwI6OdzGhampWfNX1fWrmV0ey0pJ5cu5ULvvccP7zxU/5yR/XcLSpOdphGWN6SVII29wAzAcmicg2oBK4OqJRxZHt+w5xJEYLSXZXWnIiv55zGoW5GTzwegVb9xzkgatPJystOdqhGWMiLJQzFVXVLwL5wCRVPSfE/UwIYuURwuGWkCD8aMYk7p09hfcravj6g+9RWV0f7bCMMREWSnJ4HkBV61W15Q63xZELKb5UuHtU+svlr7a+WTqKJ+dOpbqugSt+/Q6vWoVjY/q1dpOKiEwSka8D2SLytYCv7wBpvRZhP+fz15GdnkxuZkq0Q4mYs4vyeOGmcyjMy+S7T5bxy5c30NRsA/jG9EcdjalMBL4MDAIuD2g/AHwvkkHFE+8RwpkxX0iypwoGZ/C775/Fv/5xDfe/Vs7qbbXM++apDO7HydSYeNRuUlHVPwF/EpGzVPX9Xowprvj89ZxbHDuPNY6ktORE7p09hVNGDeLfX1jLpb96m/vmnMq0cbnRDs0YEyahjKl8KCI3iMiDIrKg5SvikcWBlkKSRUP653hKMCLC300bwx+u/wLpKYlc9fAyfvnyBhpt2rEx/UIoSeUpYBgwA3gT73nxVpI2DFoKSfaVkvfhdPLIbP580znMPr2A+18r58r5y6jaezDaYRljeiiUpDJeVf8VqFfVJ4AvAZ+PbFjxoaWQ5Pg4OlMJlJmaxM+/cQq/mnMqG3ce4NL73ua5lVvsLnxj+rBQkspR932fiJwMZANDIhdS/KjY7QpJ5sRnUmkx69SRvHjzuZSMyOLHz3/Cdx5byY7aQ9EOyxjTDaEklfkiMhj4F2AJsA64J6JRxQlfdR2jczJISbJ7SUflZPDs96Zx16zJrKjcwyW/fItFK7faWYsxfUynv81U9RFV3auqb6nqOFUdAvw1lM5FZKaIbBCRchG5Lcj6VBF5zq1fLiKFAetud+0bRGRGZ32KyHQR+UBEPhKRd0RkfCgxRlPF7nrG5cX3WUqghATh22cVsvSfzqNkRBa3Pr+aby9YwWc1die+MX1Fh0lFRM4SkdkiMsS9niIizwDvdtaxiCQCDwCXAiXAVSJS0mazucBeVR0PzMOdAbnt5gCTgZnAgyKS2EmfvwWuVtVTgWfwzqxiVlOzUlnTfwpJhtPo3GNnLR9u2cfF897iV69uoqGxKdqhGWM60dEd9T8HFgBfB/4iIj8FXgaWA8Uh9D0VKFdVn6oeARYCs9psMwt4wi0vBqaLdxfgLGChqjaoaiVQ7vrrqE8FstxyNhDTD/RoKSTZ32p+hUvLWcvffng+l5QMZd6rG7n0vrd5t7w62qEZYzrQ0R31XwJOU9XDbkxlK3Cyqm4Ose+Rbp8WVZw4a6x1G1VtFJFaINe1L2uz70i33F6f3wVeFJFDwH5gWrCgROQ64DqA0aNHh/hWwq/cFZLsT9WJI2FoVhq/+dbpfLPUz7/+aQ1XP7KcL08Zzm2XTqJgcN98UqYx/VlHl78Oq+phAFXdC2zqQkKJhh8Al6lqAfAY8MtgG6nqfFUtVdXS/Pzo3cneco9KX3wufTScNyGfpf90HjdPL+aVdbu46Bdvcu9Ln1LXYM+LMyaWdHSmMk5ElgS8Hhv4WlWv6KTvbcCogNcFri3YNlUikoR32aqmk31PaBeRfOAUVV3u2p8DXuokvqiqcIUkc6z2VcjSkhP5wcUTuPLMUdz70qc8+EYFi8qq+NGMCcw+YxSJCf27fpoxfUFHSaXt+Mcvutj3SqBYRMbiJYQ5wLfabLMEuAZ4H5gNvKaq6pLXMyLyS2AE3hjOCkDa6XMvXjXlCaq6EbgYWN/FeHuVL04KSUbCiEHp3DfnNK45u5D/+PM6fvz8Jzz+3mfcOmMiF0zMt8/UmCjqqKDkmz3p2I2R3AgsBRKBBaq6VkTuAspUdQnwKPCUiJQDe/CSBG67RXj3xDQCN6hqE0CwPl3794DnRaQZL8lc25P4I63CX8/5E+KjkGSknDZ6MM//w9m8sHoHP1/6KX//+EpKxwzmRzMm8nkrUmlMVEg831xWWlqqZWVlvX7cA4eP8rk7X+bWmRO5/oKYv52mTzjS2Myisq38+rVN7NrfwLnFefxoxkSmFAyKdmjG9DsiskpVS4Ots1u5o+DYIL3N/AqXlKQE/m7aGN780YX85LKTWLOtlit+8y7fe7KMj7fui3Z4xsSNjsZUTIQcey69zfwKt7TkRL533jjmTB3Fgnc2s+DdSmate5dzi/O48cLxdlnMmAjrNKmIyAt4NxYGqgXKgP9pmXZsQufzWyHJSBuYlszNXyxm7rlj+d9ln/HI2z6unL+MMwsHc8OF4zl/gg3oGxMJoVz+8gF1wMPuaz/e81QmuNemiyr8VkiytwxITeL75xfxzo8v4s7LS6jae4jvPLaSy+5/h8Wrqqz0izFhFsrlr7NV9cyA1y+IyEpVPVNE1kYqsP7M57dCkr0tLTmR73xhLN/6/Bj++OE2Hn7bxz//7mPueelTvj1tDFdPG2P3DBkTBqH8qTxARFrrmbjllhHmIxGJqh9rKSRZNMQG6aMhJSmBb545ipd/cB5PXjuVk4Zn8YtXNnLWf/2N23+/mk277KGmxvREKGcqPwTeEZEKvJsPxwLXi0gmx4pBmhBt2+sVkrQzlegSEc6bkM95E/LZtOsAC96t5PkPtvHsiq1MG5fD1Z8fw4zJw+wSpTFd1GlSUdUXRaQYmOSaNgQMzt8Xscj6qQr3CGE7U4kdxUMH8l9fm8I/XzKR58q28szyLdz07IfkDUjhm6WjuGrqaEblWPFKY0IR6pTiM4BCt/0pIoKqPhmxqPqxit2uOrGdqcSc3AGpXH/BeL5/XhFvbfLzv8u28NCbFfz2zQrOn5DPt6aO5sJJQ0hOtLMXY9oTypTip4Ai4COgZaqMApZUusFXXc+gDCskGcsSEoQLJg7hgolD2L7vEAtXbmXhii1c99QqcjNT+MppI/n66QWUjMjqvDNj4kwoZyqlQInGcz2XMKrYXce4PCsk2VeMGJTOLRdP4KaLxvPmBj+LV1Xx5PubefSdSkqGZ/H1MwqYdeoI8gakRjtUY2JCKEllDTAM2BHhWOKCr9oKSfZFyYkJfLFkKF8sGcre+iO8sHo7i1dV8R9/Xsd/vbieCyYO4SunjWD6pKGkpyRGO1xjoiaUpJIHrBORFUBDS2MIz1Mxbew/fBT/gQar+dXHDc5M4dtnFfLtswrZuOsAz6+q4g8fbuPV9bvISEnkiycN5fJTRnDehDxSkyzBmPgSSlK5M9JBxIuWQpLjrOZXvzFh6EBuv+wkbp05iRWVe3hh9Xb++skOlny8nYFpScyYPIzLTxnB2UW5NsBv4kIoU4p79FwVc4yvtZCknan0N4kJwllFuZxVlMu/XzGZ9ypqeOHj7Sxds5PFq6oYnJHM9JOGMmPyMM4tziMt2c5gTP/UblIRkXdU9RwROcDxBSUFUFW1qS9dVOGvc4Uk7Z6H/iw5MYHzJ+Rz/oR8fvqVk3lro58XP9nB0rVegklPTuT8CflcMnko0ycNJTsjOdohGxM2HT358Rz3fWDvhdO/+fz1VkgyzqQlJ3LJ5GFcMnkYRxqbWV5Zw9K1O3l57S5eWruTpATh8+NymDF5GBdNGkLBYPuDw/RtIT35UUQSgaEEJCFV3RLBuHpFbz/58ZJ5bzI6J4NHrjmz841Nv9bcrHxctY+X1+1i6dqdreNtxUMGcOGkIVwwMZ/SMTn2B4iJSR09+TGUmx9vAv4N2AU0u2YFpoQtwjjQ1KxsrjnIBROHRDsUEwMSEoTTRg/mtNGD+fHMSVT463j90928scHPY+9WMv8tHwNSkzhnfB4XTsrngolDGJqVFu2wjelUKLO/bgYmqmpNVzsXkZnAr4BE4BFV/Vmb9al4d+afAdQAV6rqZrfudmAu3l38/6iqSzvqU7y7CX8KfMPt81tVvb+rMUdKSyFJe9qjCaYofwBF+QP47rnjqG9o5N3yal7f4OeNDbt5ae1OAE4ansW5xXl8YXweUwtz7H4YE5NCSSpb8Z702CXuktkDwMVAFbBSRJao6rqAzeYCe1V1vIjMAe4BrhSREmAOMBkYAbwqIhPcPu31+R1gFDBJVZtFJKZOCVoeITzOZn6ZTmSmJrWOw6gqG3fV8fqG3byxYTePv7uZ+W/5SElM4PQxgzhnvJdkPjcymySbsmxiQChJxQe8ISJ/4fibH3/ZyX5TgXJV9QGIyEJgFhCYVGZx7D6YxcBv3BnHLGChqjYAlSJS7vqjgz7/AfiWqja7+HaH8N56TYVNJzbdICJMHDaQicMG8v3zizh0pIkVm/fwbnk172yq5r9f3sh/v7yRgalJTCvK5QtFuUwrymXCkIEkJFgpINP7QkkqW9xXivsK1Ui8s5wWVcDn29tGVRtFpBbIde3L2uw70i2312cR3lnOVwE/3iWzTW2DEpHrgOsARo8e3XZ1xFT4rZCk6bn0lMTW6coANXUNvFdRw7vl1by9qZpX1u0CIDs9mTMLc5g2LoepY3MoGZ5lZzKmV3SYVNwlrAmqenUvxdMTqcBhVS0Vka8BC4Bz226kqvOB+eDN/uqt4Hz+Oit3b8Iud0Aql58ygstPGYGqUrX3EMsr97CisoYVlXt4db2XZDJTEjmjMIfPj/W+PleQbSVkTER0mFRUtUlExohIiqp29dHB2/DGOFoUuLZg21SJSBKQjTdg39G+7bVXAb93y38AHutivBHlq67nAiskaSJIRBiVk8GonAxmn1EAwM7aw6zYfCzJ/HzpBgBSkxI4pWAQp40ZxOmjB3P66MHkD7RKy6bnQh1TeVdElgD1LY0hjKmsBIpFZCzeL/45wLfabLMEuAZ4H5gNvKaq6o71jIj8Em+gvhhYgXc3f3t9/hG4EKgEzgc2hvDeekVLIUkbpDe9bVh2GlecMoIrThkBwJ76I6yo3MOKyj18sGUvC96p5H+afACMyklvTTCnjx7MpOEDrV6Z6bJQkkqF+0oAQr673o2R3AgsxZv+u0BV14rIXUCZqi4BHgWecgPxe/CSBG67RXgD8I3ADaraBBCsT3fInwFPi8gPgDrgu6HGGmktN7bZdGITbTmZKcw8eRgzTx4GwOGjTazdXssHn+3jgy17Wear4U8fbQcgLTmBKSO9s5lTCwbxuYJsRg5Kt2cBmQ6FdEd9f9Vbd9Q/v6qKH/7uY1695XzG27PpTQxTVbbXHubDLXtbE83a7bUcbfJ+T+RkpvC5kdneV0E2UwqyGZaVZokmzvT0jvp84Fa8e0Zab+lV1YvCFmE/56u2QpKmbxARRg5KZ+SgdL48xbtk1tDYxIadB1hdVcsnVbWs3lbLb9+soKnZSzR5A1KZUpDdmmymFGQzxO7+j1uhXP56GngO+DLwfbwxEH8kg+pvKnbXM8YKSZo+KjUpkSkFg5hSMKi17fDRJtbt2O8lmapaPtm2jzc27MblGfIGpHLS8IGUDM+iZEQWJw3PYlxepk1rjgOhJJVcVX1URG52z1Z5U0RWRjqw/sRXXWcP5jL9SlpyYuuAfouDRxpZt30/q6tqWbdjP+t37OexdzdzpMkrGZiSlMDEoQM5afhAThqe1fqVnW6l//uTUJLKUfd9h4h8CdgO5EQupP6lqVnZXH2QC62QpOnnMlKSKC3MobTw2K+Ho03NVPjrWL9jP+t3HGDd9v38bf1uFpVVtW4zclA6Jw3PYsLQAUwYOpDioV4dNHuQWd8USlL5qYhkAz8Efg1kAT+IaFT9SNXegxxparYzFROXkhMTmDQsi0nDsvjqaV6bquI/0MBadzazfscBPt2xnzc27KbRXT9LECjMzaS4NdEMZMLQAYzLG2CXkWNcKI8T/rNbrMW7D8R0wbHpxDbryxjwJgMMyUpjSFbacWfwRxqbqayuZ+OuA2zadYCNu+rYuPsAr67f3TopIDFBKMzNOCHRjM3LtKrNMSKU2V8TgFBJVxoAABOuSURBVN8CQ1X1ZBGZAlyhqj+NeHT9gFUnNiY0KUkJrcUzAzU0NuHztySbOjbuOsD6Hft5ae1OAu+IGDkonXH5mYzLy2Sce5TAuPxMhmWlWXHNXhTK5a+HgR8B/wOgqqtF5Bm8Z5eYTlghSWN6JjUpsXVQP9Dho16y8VXXed/9dfiq61m8qor6I02t26UnJzI2L9NLOPkDKMrP9M5u8jMZkBrKr0DTFaF8ohmquqLNzU2NEYqn3/H56+zSlzERkJacSMkIb8pyIFVl94EGKvwtycZLPKurannxkx2t057Bm/pcmJvB6NwMxuRkUpiXweicDApzMxmUkWw3dXZDKEmlWkSK8B4hjIjMBnZENKp+pMJfz4UTrZCkMb1FRBialcbQrDTOLso7bt3ho01s2XMQn7+OCn89W2oOsrmmnvcravj9B8fXux2YlsSY3AzG5GYyxiWa0bkZjMnNYOhAu6TWnlCSyg14peInicg2vIKNfaEUftTVHjpKdV0DRVaaxZiYkJacyIShA5kw9MQyhoePNrF1z0E+c4lmy56DbK45yNpttSxds7N1Zhp4VZ5H53gJpmBwBgWD092Xt5ydHr9nOaHM/vIBXxSRTCBBVQ+IyD8B90U8uj7O1zJIb89RMSbmpSUnUuxmlbXV2NTM9n2H+WxPPZtrDrKlxvu+dc9Blvn2UNdw/IjAwNQkRgYkmcCEM2pwBlnpSf026YQ8SqWq9QEvb8GSSqdaphPbzC9j+rakxARGu7GXc4uPX6eq1B46StXeQ1TtPei+tywfZJmvptOkM3JQOsMHpTE8O50Rg9IYMjCNxD56ea27Ux/65rvtZRX+OpIShDG5VkjSmP5KRBiUkcKgjBROHpl9wvruJJ3EBGHowFSGD0pneHYaIwalMyI7jeGD0hmR7SWg3MyUmDzb6W5Sid96+V3g89czOifDHnRkTBwLJensP9zIjtpD7Nh3mO1tvq/ZVsvL63ZxpLH5uP1SkhIYnp3mJZ3sY2c6w9wkhaHZqeRlpvb6hIJ2k4qIHCB48hAgPWIR9SNeIUm79GWMaZ+IkJ2eTHZ6MpOGZQXdRlXZU3+EHbWH2b7vkPfdJZ0dtYdYXrmHnfsPt1YeaJGUIAwZmMrQ7LTWZDPMLZ9dlBuRRxS0m1RUNeSnPJoTWSFJY0y4iAi5A1LJHZAa9GwHvN851XUN7Kw9zM79h9m1//Bxyxt3HeDtTdWtl9qevHZq7yYV0zMthSTtxkdjTG9ITDh2f84pHWxX19DIztrDDM+OzIPULKlEyLGaXzad2BgTOwakJkX0seYRHUEWkZkiskFEykXktiDrU0XkObd+uYgUBqy73bVvEJEZXejzfhGpi9R7CpVNJzbGxKOIJRURSQQeAC4FSoCrRKSkzWZzgb2qOh6YB9zj9i0B5gCTgZnAgyKS2FmfIlIKDCYGVPjrGWyFJI0xcSaSZypTgXJV9anqEWAhMKvNNrOAJ9zyYmC6eBOvZwELVbVBVSuBctdfu326hPNz4NYIvqeQVfht5pcxJv5EMqmMBLYGvK5ybUG3UdVGvAeB5Xawb0d93ggsUdUOi12KyHUiUiYiZX6/v0tvqCt8/nqKbDzFGBNn+sVdeSIyAvgG3uOOO6Sq81W1VFVL8/MjUz24pZCknakYY+JNJJPKNmBUwOsC1xZ0GxFJArKBmg72ba/9NGA8UC4im4EMESkP1xvpKiskaYyJV5FMKiuBYhEZKyIpeAPvS9psswS4xi3PBl5TVXXtc9zssLFAMbCivT5V9S+qOkxVC1W1EDjoBv+joqLlufRW8t4YE2cidp+KqjaKyI3AUiARWKCqa0XkLqBMVZcAjwJPubOKPXhJArfdImAd3lMmb1DVJoBgfUbqPXSXzxWSHJ1jhSSNMfElojc/quqLwItt2u4IWD6MNxYSbN+7gbtD6TPINlE9RfD56xmda4UkjTHxx37rRUCFv45xeXbpyxgTfyyphFljUzOf1RykaIgN0htj4o8llTCr2nvIKyRpZyrGmDhkSSXMfNVWSNIYE78sqYRZSyFJK3lvjIlHllTCrMJfx+CMZAZbIUljTByypBJmFf56O0sxxsQtSyph5vPX2XiKMSZuWVIJo9qDR6muO2KFJI0xccuSShhVuJlfdvnLGBOvLKmE0bFHCNvlL2NMfLKkEkZWSNIYE+8sqYRRhb/OCkkaY+Ka/fYLI59NJzbGxDlLKmHS2NTM5pp6G08xxsQ1SyphUrX3EEeb1ApJGmPimiWVMGkpJGkl740x8cySSphU7HbTie1MxRgTxyyphImvuo6czBQrJGmMiWsRTSoiMlNENohIuYjcFmR9qog859YvF5HCgHW3u/YNIjKjsz5F5GnXvkZEFohIciTfW1sVu+sZl2eXvowx8S1iSUVEEoEHgEuBEuAqESlps9lcYK+qjgfmAfe4fUuAOcBkYCbwoIgkdtLn08Ak4HNAOvDdSL23YHzVVkjSGGMieaYyFShXVZ+qHgEWArPabDMLeMItLwami4i49oWq2qCqlUC566/dPlX1RXWAFUBBBN/bcVoKSdo9KsaYeBfJpDIS2Brwusq1Bd1GVRuBWiC3g3077dNd9vo/wEs9fgchqmh9hLAlFWNMfOuPA/UPAm+p6tvBVorIdSJSJiJlfr8/LAc89ghhu/xljIlvkUwq24BRAa8LXFvQbUQkCcgGajrYt8M+ReTfgHzglvaCUtX5qlqqqqX5+fldfEvBVbhCkqOskKQxJs5FMqmsBIpFZKyIpOANvC9ps80S4Bq3PBt4zY2JLAHmuNlhY4FivHGSdvsUke8CM4CrVLU5gu/rBD5/HWOskKQxxpAUqY5VtVFEbgSWAonAAlVdKyJ3AWWqugR4FHhKRMqBPXhJArfdImAd0AjcoKpNAMH6dId8CPgMeN8b6+f3qnpXpN5foAp/vY2nGGMMEUwq4M3IAl5s03ZHwPJh4Bvt7Hs3cHcofbr2iL6X9jQ2NfNZTT3TTxoSjcMbY0xMses1PdRaSNLOVIwxxpJKT1X4W55LbzO/jDHGkkoPtT6X3gpJGmOMJZWeqvBbIUljjGlhSaWHfH4rJGmMMS0sqfRQhb/OBumNMcaxpNIDtQePUlN/xKoTG2OMY0mlB1oKSdqZijHGeCyp9EDF7pbqxHamYowxYEmlR3zV9SQnWiFJY4xpYUmlByp21zE6xwpJGmNMC/tt2AO+aiskaYwxgSypdFNLIUkbpDfGmGMsqXTTVldI0gbpjTHmGEsq3eTz23RiY4xpy5JKN1l1YmOMOZEllW7y+evJzUxhUIYVkjTGmBaWVLqpwl9n4ynGGNOGJZVu8qoT23iKMcYEsqTSDfsOHqGm/ghFQ+xMxRhjAkU0qYjITBHZICLlInJbkPWpIvKcW79cRAoD1t3u2jeIyIzO+hSRsa6PctdnxAY7Kuxpj8YYE1TEkoqIJAIPAJcCJcBVIlLSZrO5wF5VHQ/MA+5x+5YAc4DJwEzgQRFJ7KTPe4B5rq+9ru+IaJ1OPMSSijHGBIrkmcpUoFxVfap6BFgIzGqzzSzgCbe8GJguIuLaF6pqg6pWAuWuv6B9un0ucn3g+vxKpN5Yhd8VkhycHqlDGGNMnxTJpDIS2Brwusq1Bd1GVRuBWiC3g33ba88F9rk+2jsWACJynYiUiUiZ3+/vxtuCwtwMvnraSJKskKQxxhwn7n4rqup8VS1V1dL8/Pxu9TFn6mjunX1KmCMzxpi+L5JJZRswKuB1gWsLuo2IJAHZQE0H+7bXXgMMcn20dyxjjDERFsmkshIodrOyUvAG3pe02WYJcI1bng28pqrq2ue42WFjgWJgRXt9un1ed33g+vxTBN+bMcaYIJI636R7VLVRRG4ElgKJwAJVXSsidwFlqroEeBR4SkTKgT14SQK33SJgHdAI3KCqTQDB+nSH/DGwUER+Cnzo+jbGGNOLxPsjPz6VlpZqWVlZtMMwxpg+RURWqWppsHVxN1BvjDEmciypGGOMCRtLKsYYY8LGkooxxpiwieuBehHxA591c/c8oDqM4YSLxdU1FlfXWFxdE6txQc9iG6OqQe8ej+uk0hMiUtbe7Idosri6xuLqGoura2I1LohcbHb5yxhjTNhYUjHGGBM2llS6b360A2iHxdU1FlfXWFxdE6txQYRiszEVY4wxYWNnKsYYY8LGkooxxpiwsaTSDSIyU0Q2iEi5iNzWC8fbLCKfiMhHIlLm2nJE5BUR2eS+D3btIiL3u9hWi8jpAf1c47bfJCLXtHe8TmJZICK7RWRNQFvYYhGRM9x7LXf7Sg/iulNEtrnP7SMRuSxg3e3uGBtEZEZAe9CfrXvcwnLX/px79EJnMY0SkddFZJ2IrBWRm2Ph8+ogrqh+Xm6/NBFZISIfu9j+vaP+xHs8xnOufbmIFHY35m7G9biIVAZ8Zqe69t78t58oIh+KyJ9j4bNCVe2rC194JfcrgHFACvAxUBLhY24G8tq03Qvc5pZvA+5xy5cBfwUEmAYsd+05gM99H+yWB3cjlvOA04E1kYgF77k509w+fwUu7UFcdwL/HGTbEvdzSwXGup9nYkc/W2ARMMctPwT8QwgxDQdOd8sDgY3u2FH9vDqIK6qfl9tWgAFuORlY7t5f0P6A64GH3PIc4LnuxtzNuB4HZgfZvjf/7d8CPAP8uaPPvrc+KztT6bqpQLmq+lT1CLAQmBWFOGYBT7jlJ4CvBLQ/qZ5leE/EHA7MAF5R1T2quhd4BZjZ1YOq6lt4z74JeyxuXZaqLlPvX/uTAX11J672zAIWqmqDqlYC5Xg/16A/W/cX40XA4iDvsaOYdqjqB275ALAeGEmUP68O4mpPr3xeLh5V1Tr3Mtl9aQf9BX6Wi4Hp7vhdirkHcbWnV36WIlIAfAl4xL3u6LPvlc/KkkrXjQS2BryuouP/kOGgwMsiskpErnNtQ1V1h1veCQztJL5Ixh2uWEa65XDGeKO7/LBA3GWmbsSVC+xT1cbuxuUuNZyG9xduzHxebeKCGPi83OWcj4DdeL90KzrorzUGt77WHT/s/w/axqWqLZ/Z3e4zmyciqW3jCvH43f1Z3gfcCjS71x199r3yWVlS6RvOUdXTgUuBG0TkvMCV7i+bmJgbHkuxAL8FioBTgR3AL6IRhIgMAJ4H/klV9weui+bnFSSumPi8VLVJVU8FCvD+Wp4UjTjaahuXiJwM3I4X35l4l7R+3FvxiMiXgd2quqq3jhkKSypdtw0YFfC6wLVFjKpuc993A3/A+4+2y50y477v7iS+SMYdrli2ueWwxKiqu9wvgmbgYbzPrTtx1eBdvkhq094pEUnG+8X9tKr+3jVH/fMKFlcsfF6BVHUf8DpwVgf9tcbg1me740fs/0FAXDPdpURV1QbgMbr/mXXnZ/kF4AoR2Yx3aeoi4FdE+7PqbNDFvk4YFEvCG1wby7HBq8kRPF4mMDBg+T28sZCfc/xg771u+UscP0C4wrXnAJV4g4OD3XJON2Mq5PgB8bDFwomDlZf1IK7hAcs/wLtuDDCZ4wcmfXiDku3+bIHfcfzg5/UhxCN418bva9Me1c+rg7ii+nm5bfOBQW45HXgb+HJ7/QE3cPzg86LuxtzNuIYHfKb3AT+L0r/9Czg2UB/dz6o7v1Ti/QtvZsdGvGu9P4nwsca5H+bHwNqW4+FdC/0bsAl4NeAfpgAPuNg+AUoD+roWbxCuHPj7bsbzLN6lkaN411jnhjMWoBRY4/b5Da7qQzfjesoddzWwhON/af7EHWMDAbNs2vvZup/DChfv74DUEGI6B+/S1mrgI/d1WbQ/rw7iiurn5fabAnzoYlgD3NFRf0Cae13u1o/rbszdjOs195mtAf6XYzPEeu3fvtv3Ao4llah+VlamxRhjTNjYmIoxxpiwsaRijDEmbCypGGOMCRtLKsYYY8LGkooxxpiwsaRiTBeJSG5AVdqdcnxl3w6r8YpIqYjc38XjXeuq164WkTUiMsu1f0dERvTkvRgTbjal2JgeEJE7gTpV/e+AtiQ9Vnupp/0XAG/iVRWudaVV8lW1UkTewKsqXBaOYxkTDnamYkwYuOdqPCQiy4F7RWSqiLzvnnPxnohMdNtdEPDciztd4cY3RMQnIv8YpOshwAGgDkBV61xCmY13s9zT7gwp3T2P401XeHRpQCmYN0TkV267NSIyNchxjAkLSyrGhE8BcLaq3gJ8CpyrqqcBdwD/2c4+k/DKoU8F/s3V5Ar0MbALqBSRx0TkcgBVXQyUAVerV+SwEfg13rM9zgAWAHcH9JPhtrverTMmIpI638QYE6LfqWqTW84GnhCRYrySKG2TRYu/qFeMsEFEduOVwW8tga6qTSIyE68K7nRgnoicoap3tulnInAy8Ir3iAwS8crWtHjW9feWiGSJyCD1CiMaE1aWVIwJn/qA5f8AXlfVr7pnlrzRzj4NActNBPk/qd7A5wpghYi8glcN9842mwmwVlXPauc4bQdPbTDVRIRd/jImMrI5Vib8O93tRERGSMDzzfGedfKZWz6A9zhg8AoB5ovIWW6/ZBGZHLDfla79HKBWVWu7G5MxHbEzFWMi4168y1//AvylB/0kA//tpg4fBvzA9926x4GHROQQ3jNHZgP3i0g23v/t+/AqWwMcFpEPXX/X9iAeYzpkU4qN6eds6rHpTXb5yxhjTNjYmYoxxpiwsTMVY4wxYWNJxRhjTNhYUjHGGBM2llSMMcaEjSUVY4wxYfP/ARJdsyDLX689AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgkq6T_rdBU4"
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-N4k9wLdBRt"
      },
      "source": [
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
        "\n",
        "\n",
        "def accuracy_function(real, pred):\n",
        "  accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
        "\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  accuracies = tf.math.logical_and(mask, accuracies)\n",
        "\n",
        "  accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
        "  mask = tf.cast(mask, dtype=tf.float32)\n",
        "  return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjnjWWaKdBO1"
      },
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOG6QoO_dBLr"
      },
      "source": [
        "transformer = Transformer(\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    input_vocab_size=tokenizers.pt.get_vocab_size(),\n",
        "    target_vocab_size=tokenizers.en.get_vocab_size(),\n",
        "    pe_input=1000,\n",
        "    pe_target=1000,\n",
        "    rate=dropout_rate)\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04MztrTGdBIX"
      },
      "source": [
        "def create_masks(inp, tar):\n",
        "  # Encoder padding mask\n",
        "  enc_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "  # Used in the 2nd attention block in the decoder.\n",
        "  # This padding mask is used to mask the encoder outputs.\n",
        "  dec_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "  # Used in the 1st attention block in the decoder.\n",
        "  # It is used to pad and mask future tokens in the input received by\n",
        "  # the decoder.\n",
        "  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "  dec_target_padding_mask = create_padding_mask(tar)\n",
        "  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "\n",
        "  return enc_padding_mask, combined_mask, dec_padding_mask\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isBucn6JKWqt"
      },
      "source": [
        "checkpoint_path = \"./checkpoints/train\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "# if a checkpoint exists, restore the latest checkpoint.\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "  print('Latest checkpoint restored!!')\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9O5arLPkDmi"
      },
      "source": [
        "EPOCHS = 20\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HPM-HLZJT8w"
      },
      "source": [
        "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
        "# execution. The function specializes to the precise shape of the argument\n",
        "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
        "# batch sizes (the last batch is smaller), use input_signature to specify\n",
        "# more generic shapes.\n",
        "\n",
        "train_step_signature = [\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "]\n",
        "\n",
        "\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def train_step(inp, tar):\n",
        "  tar_inp = tar[:, :-1]\n",
        "  tar_real = tar[:, 1:]\n",
        "\n",
        "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions, _ = transformer(inp, tar_inp,\n",
        "                                 True,\n",
        "                                 enc_padding_mask,\n",
        "                                 combined_mask,\n",
        "                                 dec_padding_mask)\n",
        "    loss = loss_function(tar_real, predictions)\n",
        "\n",
        "  gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "  train_loss(loss)\n",
        "  train_accuracy(accuracy_function(tar_real, predictions))\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xGoPYw7Ju2R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "outputId": "ce64f7ec-93e9-49d6-d896-974950659e1e"
      },
      "source": [
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "\n",
        "  # inp -> portuguese, tar -> english\n",
        "  for (batch, (inp, tar)) in enumerate(train_batches):\n",
        "    train_step(inp, tar)\n",
        "\n",
        "    if batch % 50 == 0:\n",
        "      print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print(f'Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}')\n",
        "\n",
        "  print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "\n",
        "  print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-d5f75ec190c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;31m# inp -> portuguese, tar -> english\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    948\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3024\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1961\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Input 'pred' passed float expected bool while building NodeDef 'transformer_1/encoder_2/encoder_layer_5/multi_head_attention_15/dropout_44/cond/switch_pred/_2' using Op<name=Switch; signature=data:T, pred:bool -> output_false:T, output_true:T; attr=T:type> [Op:__inference_train_step_22049]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaU0sfX-RFkV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLpTE5c9pK1d"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCKMxlDRRqug"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3tgzA3LR15s"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVv2BpkfR585"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WblxTTIdS-el"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGm622rxnVK3"
      },
      "source": [
        "# deneme = np.triu(np.ones([3,5,6]))\n",
        "\n",
        "\n",
        "# mha =MultiHeadAttention(head_size=128, num_heads=12, return_attn_coef=True)\n",
        "# query = np.random.rand(3, 5, 4) # (batch_size, query_elements, query_depth)\n",
        "# key = np.random.rand(3, 6, 5) # (batch_size, key_elements, key_depth)\n",
        "# value = np.random.rand(3, 6, 6) # (batch_size, key_elements, value_depth)\n",
        "# attention = mha([query, key, value],\n",
        "#                 mask = 1-deneme,\n",
        "                \n",
        "#                 ) # (batch_size, query_elements, value_depth)\n",
        "# attention\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nAaN5lvTCwb"
      },
      "source": [
        "\n",
        "# #@This part has been tcomment out on porpose. Becase, both tfa and tf packages are using the same class\n",
        "# #@name for MultiHeadAttention. At this notebook, I will use the MHA from the tensorflow addons.\n",
        "# from tensorflow import keras \n",
        "# from tensorflow.keras import layers\n",
        "# from tensorflow.keras.layers import MultiHeadAttention  \n",
        "\n",
        "# query = np.random.rand(3, 5, 4) # (batch_size, query_elements, query_depth)\n",
        "# value = np.random.rand(3, 6, 6) # (batch_size, key_elements, value_depth)\n",
        "# key = np.random.rand(3, 6, 5) # (batch_size, key_elements, key_depth)\n",
        "\n",
        "# layer = MultiHeadAttention(num_heads=2, key_dim=2)\n",
        "# target = tf.keras.Input(shape=[8, 16])\n",
        "# source = tf.keras.Input(shape=[4, 16])\n",
        "# # output_tensor, weights = layer(query, key,return_attention_scores=True)\n",
        "\n",
        "\n",
        "# layer(query, key,value, attention_mask =np.ones([3,5,6]),return_attention_scores = True)\n",
        "# # print(output_tensor.shape)\n",
        "\n",
        "# # print(weights.shape)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbhGgRbQTbKz"
      },
      "source": [
        "deneme.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHqkeHTVUVj7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3mjEMM9YYKo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}