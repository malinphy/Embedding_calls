{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"roberta trial.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMUhpm4D8dRFyL8Foh6tBB6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"pDdQHXEbQhH1","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1627557245493,"user_tz":-180,"elapsed":10577,"user":{"displayName":"mehmetali nebioglu","photoUrl":"","userId":"08430993529930152583"}},"outputId":"5c457d55-8fa1-4626-9a51-bdfefa809575"},"source":["import numpy as np \n","import pandas as pd\n","import tensorflow as tf\n","import tensorflow.keras.backend as K\n","from sklearn.model_selection import StratifiedKFold\n","!pip install transformers==2.8 \n","import tokenizers\n","import json"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting transformers==2.8\n","  Downloading transformers-2.8.0-py3-none-any.whl (563 kB)\n","\u001b[?25l\r\u001b[K     |▋                               | 10 kB 29.2 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 20 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 30 kB 25.1 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 40 kB 18.9 MB/s eta 0:00:01\r\u001b[K     |███                             | 51 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 61 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |████                            | 71 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 81 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 92 kB 10.8 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 102 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 112 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |███████                         | 122 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 133 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 143 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 153 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 163 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 174 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 184 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 194 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 204 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 215 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 225 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 235 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 245 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 256 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 266 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 276 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 286 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 296 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 307 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 317 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 327 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 337 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 348 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 358 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 368 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 378 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 389 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 399 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 409 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 419 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 430 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 440 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 450 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 460 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 471 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 481 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 491 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 501 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 512 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 522 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 532 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 542 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 552 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 563 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 563 kB 9.1 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==2.8) (2019.12.20)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==2.8) (1.19.5)\n","Collecting boto3\n","  Downloading boto3-1.18.9-py3-none-any.whl (131 kB)\n","\u001b[K     |████████████████████████████████| 131 kB 19.3 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==2.8) (4.41.1)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 19.5 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==2.8) (2.23.0)\n","Collecting tokenizers==0.5.2\n","  Downloading tokenizers-0.5.2-cp37-cp37m-manylinux1_x86_64.whl (5.6 MB)\n","\u001b[K     |████████████████████████████████| 5.6 MB 33.0 MB/s \n","\u001b[?25hCollecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 66.4 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==2.8) (3.0.12)\n","Collecting s3transfer<0.6.0,>=0.5.0\n","  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n","\u001b[K     |████████████████████████████████| 79 kB 9.9 MB/s \n","\u001b[?25hCollecting botocore<1.22.0,>=1.21.9\n","  Downloading botocore-1.21.9-py3-none-any.whl (7.8 MB)\n","\u001b[K     |████████████████████████████████| 7.8 MB 58.6 MB/s \n","\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n","  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n","Collecting urllib3<1.27,>=1.25.4\n","  Downloading urllib3-1.26.6-py2.py3-none-any.whl (138 kB)\n","\u001b[K     |████████████████████████████████| 138 kB 71.5 MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.22.0,>=1.21.9->boto3->transformers==2.8) (2.8.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.22.0,>=1.21.9->boto3->transformers==2.8) (1.15.0)\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 73.6 MB/s \n","\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.8) (2021.5.30)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.8) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.8) (2.10)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.8) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.8) (1.0.1)\n","Installing collected packages: urllib3, jmespath, botocore, s3transfer, tokenizers, sentencepiece, sacremoses, boto3, transformers\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","Successfully installed boto3-1.18.9 botocore-1.21.9 jmespath-0.10.0 s3transfer-0.5.0 sacremoses-0.0.45 sentencepiece-0.1.96 tokenizers-0.5.2 transformers-2.8.0 urllib3-1.25.11\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["urllib3"]}}},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"EUaHcei9QhEd","executionInfo":{"status":"ok","timestamp":1627557245499,"user_tz":-180,"elapsed":27,"user":{"displayName":"mehmetali nebioglu","photoUrl":"","userId":"08430993529930152583"}}},"source":["from google.colab import files \n","from google.colab import drive"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"upnKME-FQhBg","executionInfo":{"status":"ok","timestamp":1627557273788,"user_tz":-180,"elapsed":28312,"user":{"displayName":"mehmetali nebioglu","photoUrl":"","userId":"08430993529930152583"}},"outputId":"3477361b-d833-4be9-ec58-2bd1f03eaeb5"},"source":["drive.mount('/content/drive')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"W8LThZ_6Q1zN","executionInfo":{"status":"ok","timestamp":1627557315205,"user_tz":-180,"elapsed":300,"user":{"displayName":"mehmetali nebioglu","photoUrl":"","userId":"08430993529930152583"}}},"source":["# path = '/content/drive/MyDrive/train.txt'\n","path2 = '/content/drive/MyDrive/Colab Notebooks/shared_alp/roberta/'"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"BfMlBIdFVFHD","executionInfo":{"status":"ok","timestamp":1627557325649,"user_tz":-180,"elapsed":278,"user":{"displayName":"mehmetali nebioglu","photoUrl":"","userId":"08430993529930152583"}}},"source":[""],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"18srsNqiVTrK","executionInfo":{"status":"ok","timestamp":1627557327870,"user_tz":-180,"elapsed":275,"user":{"displayName":"mehmetali nebioglu","photoUrl":"","userId":"08430993529930152583"}}},"source":["MAX_LEN = 96\n","# PATH = '../input/tf-roberta/'\n","tokenizer = tokenizers.ByteLevelBPETokenizer(\n","    vocab_file=path2+'vocab-roberta-base.json', \n","    merges_file=path2+'merges-roberta-base.txt', \n","    lowercase=True,\n","    add_prefix_space=True\n",")\n","sentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}\n","# train = pd.read_csv('../input/tweet-sentiment-extraction/train.csv').fillna('')\n","# train.head()"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"H93Mdy4UYGVq","executionInfo":{"status":"ok","timestamp":1627557331255,"user_tz":-180,"elapsed":749,"user":{"displayName":"mehmetali nebioglu","photoUrl":"","userId":"08430993529930152583"}},"outputId":"84834aae-aea7-4d83-a873-ed470be8710a"},"source":["train = pd.read_csv('https://raw.githubusercontent.com/malinphy/datasets/main/tweet_sentiment_extraction/train.txt').fillna('')\n","train.head()"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>textID</th>\n","      <th>text</th>\n","      <th>selected_text</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>cb774db0d1</td>\n","      <td>I`d have responded, if I were going</td>\n","      <td>I`d have responded, if I were going</td>\n","      <td>neutral</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>549e992a42</td>\n","      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n","      <td>Sooo SAD</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>088c60f138</td>\n","      <td>my boss is bullying me...</td>\n","      <td>bullying me</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>9642c003ef</td>\n","      <td>what interview! leave me alone</td>\n","      <td>leave me alone</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>358bd9e861</td>\n","      <td>Sons of ****, why couldn`t they put them on t...</td>\n","      <td>Sons of ****,</td>\n","      <td>negative</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       textID  ... sentiment\n","0  cb774db0d1  ...   neutral\n","1  549e992a42  ...  negative\n","2  088c60f138  ...  negative\n","3  9642c003ef  ...  negative\n","4  358bd9e861  ...  negative\n","\n","[5 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"M3sdnpulYO4I","executionInfo":{"status":"ok","timestamp":1627557340614,"user_tz":-180,"elapsed":6370,"user":{"displayName":"mehmetali nebioglu","photoUrl":"","userId":"08430993529930152583"}}},"source":["ct = train.shape[0]\n","input_ids = np.ones((ct,MAX_LEN),dtype='int32')\n","attention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\n","token_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\n","start_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n","end_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n","\n","for k in range(train.shape[0]):\n","    \n","    # FIND OVERLAP\n","    text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n","    text2 = \" \".join(train.loc[k,'selected_text'].split())\n","    idx = text1.find(text2)\n","    chars = np.zeros((len(text1)))\n","    chars[idx:idx+len(text2)]=1\n","    if text1[idx-1]==' ': chars[idx-1] = 1 \n","    enc = tokenizer.encode(text1) \n","        \n","    # ID_OFFSETS\n","    offsets = []; idx=0\n","    for t in enc.ids:\n","        w = tokenizer.decode([t])\n","        offsets.append((idx,idx+len(w)))\n","        idx += len(w)\n","    \n","    # START END TOKENS\n","    toks = []\n","    for i,(a,b) in enumerate(offsets):\n","        sm = np.sum(chars[a:b])\n","        if sm>0: toks.append(i) \n","        \n","    s_tok = sentiment_id[train.loc[k,'sentiment']]\n","    input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n","    attention_mask[k,:len(enc.ids)+5] = 1\n","    if len(toks)>0:\n","        start_tokens[k,toks[0]+1] = 1\n","        end_tokens[k,toks[-1]+1] = 1"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"ivSvRRNVbvIe","executionInfo":{"status":"ok","timestamp":1627557341027,"user_tz":-180,"elapsed":418,"user":{"displayName":"mehmetali nebioglu","photoUrl":"","userId":"08430993529930152583"}}},"source":["test = pd.read_csv('https://raw.githubusercontent.com/malinphy/datasets/main/tweet_sentiment_extraction/test.txt').fillna('')\n","\n","ct = test.shape[0]\n","input_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\n","attention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\n","token_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n","\n","for k in range(test.shape[0]):\n","        \n","    # INPUT_IDS\n","    text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n","    enc = tokenizer.encode(text1)                \n","    s_tok = sentiment_id[test.loc[k,'sentiment']]\n","    input_ids_t[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n","    attention_mask_t[k,:len(enc.ids)+5] = 1"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"ch3WxY0LcyNO","executionInfo":{"status":"ok","timestamp":1627557345284,"user_tz":-180,"elapsed":4264,"user":{"displayName":"mehmetali nebioglu","photoUrl":"","userId":"08430993529930152583"}}},"source":["from transformers import RobertaConfig, RobertaModel,TFRobertaModel"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"3VHH5wwbcBvt","executionInfo":{"status":"ok","timestamp":1627557345286,"user_tz":-180,"elapsed":13,"user":{"displayName":"mehmetali nebioglu","photoUrl":"","userId":"08430993529930152583"}}},"source":["def build_model():\n","    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n","    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n","    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n","\n","    config = RobertaConfig.from_pretrained(path2+'config-roberta-base.json')\n","    bert_model = TFRobertaModel.from_pretrained(path2+'pretrained-roberta-base.h5',config=config)\n","    # bert_model = RobertaModel.from_pretrained(path2+'pretrained-roberta-base.h5',config=config)\n","    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n","    \n","    x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n","    x1 = tf.keras.layers.Conv1D(1,1)(x1)\n","    x1 = tf.keras.layers.Flatten()(x1)\n","    x1 = tf.keras.layers.Activation('softmax')(x1)\n","    \n","    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n","    x2 = tf.keras.layers.Conv1D(1,1)(x2)\n","    x2 = tf.keras.layers.Flatten()(x2)\n","    x2 = tf.keras.layers.Activation('softmax')(x2)\n","\n","    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n","    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n","    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n","\n","    return model"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"AmaNJ-WKcEqI","executionInfo":{"status":"ok","timestamp":1627557345287,"user_tz":-180,"elapsed":12,"user":{"displayName":"mehmetali nebioglu","photoUrl":"","userId":"08430993529930152583"}}},"source":["def jaccard(str1, str2): \n","    a = set(str1.lower().split()) \n","    b = set(str2.lower().split())\n","    if (len(a)==0) & (len(b)==0): return 0.5\n","    c = a.intersection(b)\n","    return float(len(c)) / (len(a) + len(b) - len(c))"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2EsWrK5NcGv4","executionInfo":{"status":"ok","timestamp":1627560021106,"user_tz":-180,"elapsed":2638290,"user":{"displayName":"mehmetali nebioglu","photoUrl":"","userId":"08430993529930152583"}},"outputId":"fb7332de-6a9c-4b12-ce6d-74ed9b49570a"},"source":["jac = []; VER='v0'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\n","oof_start = np.zeros((input_ids.shape[0],MAX_LEN))\n","oof_end = np.zeros((input_ids.shape[0],MAX_LEN))\n","preds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\n","preds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n","\n","skf = StratifiedKFold(n_splits=2,shuffle=True,random_state=777)\n","for fold,(idxT,idxV) in enumerate(skf.split(input_ids,train.sentiment.values)):\n","\n","    print('#'*25)\n","    print('### FOLD %i'%(fold+1))\n","    print('#'*25)\n","    \n","    K.clear_session()\n","    model = build_model()\n","        \n","    sv = tf.keras.callbacks.ModelCheckpoint(\n","        '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n","        save_weights_only=True, mode='auto', save_freq='epoch')\n","        \n","    model.fit([input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]], [start_tokens[idxT,], end_tokens[idxT,]], \n","        epochs=3, batch_size=32, verbose=DISPLAY, callbacks=[sv],\n","        validation_data=([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]], \n","        [start_tokens[idxV,], end_tokens[idxV,]]))\n","    \n","    print('Loading model...')\n","    model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n","    \n","    print('Predicting OOF...')\n","    oof_start[idxV,],oof_end[idxV,] = model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n","    \n","    print('Predicting Test...')\n","    preds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n","    preds_start += preds[0]/skf.n_splits\n","    preds_end += preds[1]/skf.n_splits\n","    \n","    # DISPLAY FOLD JACCARD\n","    all = []\n","    for k in idxV:\n","        a = np.argmax(oof_start[k,])\n","        b = np.argmax(oof_end[k,])\n","        if a>b: \n","            st = train.loc[k,'text'] # IMPROVE CV/LB with better choice here\n","        else:\n","            text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n","            enc = tokenizer.encode(text1)\n","            st = tokenizer.decode(enc.ids[a-1:b])\n","        all.append(jaccard(st,train.loc[k,'selected_text']))\n","    jac.append(np.mean(all))\n","    print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n","    print()"],"execution_count":14,"outputs":[{"output_type":"stream","text":["#########################\n","### FOLD 1\n","#########################\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n","Instructions for updating:\n","The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n","Epoch 1/3\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","430/430 [==============================] - 412s 857ms/step - loss: 2.3693 - activation_loss: 1.1608 - activation_1_loss: 1.2085 - val_loss: 1.7273 - val_activation_loss: 0.8760 - val_activation_1_loss: 0.8513\n","\n","Epoch 00001: val_loss improved from inf to 1.72726, saving model to v0-roberta-0.h5\n","Epoch 2/3\n","430/430 [==============================] - 368s 856ms/step - loss: 1.7084 - activation_loss: 0.8644 - activation_1_loss: 0.8440 - val_loss: 1.7396 - val_activation_loss: 0.8679 - val_activation_1_loss: 0.8718\n","\n","Epoch 00002: val_loss did not improve from 1.72726\n","Epoch 3/3\n","430/430 [==============================] - 368s 857ms/step - loss: 1.5332 - activation_loss: 0.7813 - activation_1_loss: 0.7519 - val_loss: 1.7055 - val_activation_loss: 0.8612 - val_activation_1_loss: 0.8442\n","\n","Epoch 00003: val_loss improved from 1.72726 to 1.70547, saving model to v0-roberta-0.h5\n","Loading model...\n","Predicting OOF...\n","430/430 [==============================] - 90s 204ms/step\n","Predicting Test...\n","111/111 [==============================] - 22s 202ms/step\n",">>>> FOLD 1 Jaccard = 0.7066549313112489\n","\n","#########################\n","### FOLD 2\n","#########################\n","Epoch 1/3\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","430/430 [==============================] - 385s 863ms/step - loss: 2.5061 - activation_loss: 1.2066 - activation_1_loss: 1.2995 - val_loss: 1.7313 - val_activation_loss: 0.8789 - val_activation_1_loss: 0.8524\n","\n","Epoch 00001: val_loss improved from inf to 1.73135, saving model to v0-roberta-1.h5\n","Epoch 2/3\n","430/430 [==============================] - 368s 856ms/step - loss: 1.7219 - activation_loss: 0.8737 - activation_1_loss: 0.8482 - val_loss: 1.6782 - val_activation_loss: 0.8560 - val_activation_1_loss: 0.8222\n","\n","Epoch 00002: val_loss improved from 1.73135 to 1.67818, saving model to v0-roberta-1.h5\n","Epoch 3/3\n","430/430 [==============================] - 367s 854ms/step - loss: 1.5525 - activation_loss: 0.7961 - activation_1_loss: 0.7564 - val_loss: 1.6530 - val_activation_loss: 0.8484 - val_activation_1_loss: 0.8046\n","\n","Epoch 00003: val_loss improved from 1.67818 to 1.65303, saving model to v0-roberta-1.h5\n","Loading model...\n","Predicting OOF...\n","430/430 [==============================] - 89s 202ms/step\n","Predicting Test...\n","111/111 [==============================] - 22s 202ms/step\n",">>>> FOLD 2 Jaccard = 0.7040629230247398\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lKy52SEcc76l"},"source":[""]},{"cell_type":"code","metadata":{"id":"m58ySf9rcJfE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627560021112,"user_tz":-180,"elapsed":43,"user":{"displayName":"mehmetali nebioglu","photoUrl":"","userId":"08430993529930152583"}},"outputId":"26289fa1-4aef-4fa5-ba1d-32640e677522"},"source":["print('>>>> OVERALL 5Fold CV Jaccard =',np.mean(jac))"],"execution_count":15,"outputs":[{"output_type":"stream","text":[">>>> OVERALL 5Fold CV Jaccard = 0.7053589271679943\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5XkaGnQ367lP"},"source":[""],"execution_count":null,"outputs":[]}]}