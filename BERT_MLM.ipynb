{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_MLM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMHXZ8jShy+VsiMQ5HMFnvR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/malinphy/Embedding_calls/blob/main/BERT_MLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohty4PVDUm8m"
      },
      "source": [
        "import numpy as np \r\n",
        "import pandas as pd\r\n",
        "import random \r\n",
        "import re\r\n",
        "import tensorflow as tf \r\n",
        "from tensorflow import keras \r\n",
        "from tensorflow.keras import layers\r\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
        "\r\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\r\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
        "\r\n",
        "\r\n",
        "from tensorflow.keras.layers import Dense, Embedding, Flatten ,MultiHeadAttention,Dropout\r\n",
        "from tensorflow.keras import Sequential\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYyI72eIUn9I"
      },
      "source": [
        "## 1) Sentences will be tokenized using the TextVectorization \r\n",
        "## 2) 12.5 % of the tokens will be replaced with the [MASK] token. Substituded tokens will be \r\n",
        "## chosen randomly. To select the random tokens, random sampling will be chosen \r\n",
        "\r\n",
        "## 3) segment id will be determined at the second step\r\n",
        "## 4) positional embeddings will be directly taken from the article"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T95t9xM_UpXD"
      },
      "source": [
        "max_length =  128  ## 256\r\n",
        "corpus_length = 128   ## 30000\r\n",
        "embedding_dimension = 128   ## 128\r\n",
        "vocab_size = 128  ## 30000\r\n",
        "number_heads = 8\r\n",
        "\r\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZXhaqbCUtkC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a01d4a37-89ca-4ff1-ba5d-f52021f913ba"
      },
      "source": [
        "trial_sentences = [\r\n",
        "    'my name is john',\r\n",
        "    'london is the capital is the england',\r\n",
        "    'weather is cold at the northern hemisphere of the world',\r\n",
        "    'today, i will go the school'\r\n",
        "]\r\n",
        "\r\n",
        "longest_sentence = []\r\n",
        "for i in trial_sentences:\r\n",
        "    longest_sentence.append(len(i.split()))\r\n",
        "    \r\n",
        "print(np.max(longest_sentence))\r\n",
        "\r\n",
        "longest_sentence = np.max(longest_sentence)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05Hg842CUvEZ"
      },
      "source": [
        "def masking_procedure (text,tok):\r\n",
        "    \r\n",
        "    # tf_tokenizer = Tokenizer()\r\n",
        "    tf_tokenizer = tok\r\n",
        "    tf_tokens=tf_tokenizer.fit_on_texts(text)\r\n",
        "\r\n",
        "    corpus = tf_tokenizer.word_index  ### TOKEN SET WITH TOKEN IDS WITHOUT THE SPECIAL TOKENS \r\n",
        "    extended_corpus = corpus.copy()   ### TOKEN SET WITH SPECIAL TOKENS\r\n",
        "\r\n",
        "    extended_corpus['MASK'] = len(corpus)+1\r\n",
        "\r\n",
        "    corpus_tokens = []   #### TOKEN SET WITHOUT THE SPECIAL TOKENS\r\n",
        "    for i,j in enumerate(corpus):\r\n",
        "        corpus_tokens.append(j)\r\n",
        "#         print(i,j)\r\n",
        "\r\n",
        "        \r\n",
        "    extended_tokens = []\r\n",
        "    for i,j in  enumerate(extended_corpus):\r\n",
        "    #     print(i,j)\r\n",
        "        extended_tokens.append(j)\r\n",
        "    # print('maskelenmis_corpus')\r\n",
        "    # print('-------')\r\n",
        "    subs_tokens=random.sample(corpus_tokens,int(len(corpus_tokens)*0.12)) ## RANDOMLY SELECTED TOKENS TO BE MASKED\r\n",
        "#     print(subs_tokens)\r\n",
        "\r\n",
        "    sequences = tf_tokenizer.texts_to_sequences(text)\r\n",
        "    padded_sequences= pad_sequences(sequences)\r\n",
        "    padded_sequences\r\n",
        "\r\n",
        "    substitute_padded_sequences= padded_sequences.copy()\r\n",
        "    actual_padded_sequences = padded_sequences.copy()\r\n",
        "\r\n",
        "    subs_tokens_ids = [ ]  ## ID OF THE SUBS TOKENS\r\n",
        "\r\n",
        "    for i in subs_tokens:\r\n",
        "        subs_tokens_ids.append(tf_tokenizer.word_index[i])\r\n",
        "    \r\n",
        "    \r\n",
        "    mask_token_id = 20\r\n",
        "#     print(subs_tokens_ids)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "    for i in range(len(extended_tokens)):\r\n",
        "        if i == (subs_tokens_ids[0]-1   ):\r\n",
        "            extended_tokens[i] = '[MASK]'\r\n",
        "    \r\n",
        "        if i == (subs_tokens_ids[1]-1):\r\n",
        "            extended_tokens[i] = '[MASK]'\r\n",
        "        \r\n",
        "        \r\n",
        "        \r\n",
        "    maskeli = []\r\n",
        "    for i in range(substitute_padded_sequences.shape[0]):\r\n",
        "        for j in range(substitute_padded_sequences.shape[1]):\r\n",
        "            if substitute_padded_sequences[i][j] != 0:\r\n",
        "                x = (substitute_padded_sequences[i][j]  )\r\n",
        "                maskeli.append(\"\".join(extended_tokens[x-1]))\r\n",
        "                \r\n",
        "                \r\n",
        "    maskeli_2 = []\r\n",
        "    for i in (substitute_padded_sequences):\r\n",
        "        maskeli_2.append('\\n')\r\n",
        "        for j in range(len(i)):\r\n",
        "        \r\n",
        "            if (i[j] !=0):\r\n",
        "#             print(i[j])\r\n",
        "                maskeli_2.append(\"\".join(extended_tokens[i[j] -1]))\r\n",
        "    \r\n",
        "    \r\n",
        "    y= \" \".join(maskeli_2)\r\n",
        "\r\n",
        "    y = y.split('\\n')\r\n",
        "    y = np.delete(y,0,0)\r\n",
        "    y\r\n",
        "     \r\n",
        "    segment_ids = substitute_padded_sequences.copy()\r\n",
        "\r\n",
        "    for i in range(segment_ids.shape[0]):\r\n",
        "        for j in range(segment_ids.shape[1]):\r\n",
        "            if (segment_ids[i][j] != 0):\r\n",
        "                segment_ids[i][j] = 1\r\n",
        "    segment_ids = np.array(segment_ids)\r\n",
        "    \r\n",
        "    return (y , extended_tokens, segment_ids,substitute_padded_sequences,actual_padded_sequences)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mteujAxNQsp"
      },
      "source": [
        "tf_tokenizer = Tokenizer()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NtDnwk6Uwei"
      },
      "source": [
        "masked_text, masked_tokens,segment_ids , masked_sequence, original_sequences = masking_procedure(trial_sentences,tf_tokenizer)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWJiPeEbwwLP",
        "outputId": "d0c89de8-5fc9-48d8-b9c5-49b5e725c276"
      },
      "source": [
        "masked_sequence,original_sequences,masked_text"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[ 0,  0,  0,  0,  0,  0,  3,  4,  2,  5],\n",
              "        [ 0,  0,  0,  6,  2,  1,  7,  2,  1,  8],\n",
              "        [ 9,  2, 10, 11,  1, 12, 13, 14,  1, 15],\n",
              "        [ 0,  0,  0,  0, 16, 17, 18, 19,  1, 20]], dtype=int32),\n",
              " array([[ 0,  0,  0,  0,  0,  0,  3,  4,  2,  5],\n",
              "        [ 0,  0,  0,  6,  2,  1,  7,  2,  1,  8],\n",
              "        [ 9,  2, 10, 11,  1, 12, 13, 14,  1, 15],\n",
              "        [ 0,  0,  0,  0, 16, 17, 18, 19,  1, 20]], dtype=int32),\n",
              " array([' my [MASK] is john ', ' london is the capital is the england ',\n",
              "        ' weather is cold at the northern hemisphere of the world ',\n",
              "        ' [MASK] i will go the school'], dtype='<U57'))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51p_DKXsUxvm"
      },
      "source": [
        "MAX_LEN = 256\r\n",
        "EMBED_DIM = 128\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def get_pos_encoding_matrix(max_len, d_emb):\r\n",
        "    pos_enc = np.array(\r\n",
        "        [\r\n",
        "            [pos / np.power(10000, 2 * (j // 2) / d_emb) for j in range(d_emb)]\r\n",
        "            if pos != 0\r\n",
        "            else np.zeros(d_emb)\r\n",
        "            for pos in range(max_len)\r\n",
        "        ]\r\n",
        "    )\r\n",
        "    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2])  # dim 2i\r\n",
        "    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2])  # dim 2i+1\r\n",
        "    return pos_enc\r\n",
        "\r\n",
        "\r\n",
        "positional_embeddings = get_pos_encoding_matrix(MAX_LEN, EMBED_DIM)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKuES0kuUz-T"
      },
      "source": [
        "pos_enc = get_pos_encoding_matrix(MAX_LEN, EMBED_DIM)\r\n",
        "# pos_enc + segment_ids\r\n",
        "\r\n",
        "total_encoding = masked_sequence+segment_ids # +pos_enc  (bi sekilde positional encodingi sokmam gerek )"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEuqxovDCRpY"
      },
      "source": [
        "total_encoding.shape\r\n",
        "intro_shape = total_encoding.shape\r\n",
        "masked_sequence = masked_sequence.astype(dtype='float32')\r\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kP_jJ8FiPoVz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c307746a-587f-4b46-db28-3d64533d3932"
      },
      "source": [
        "def encoder_structure (masked_sequence):\r\n",
        "  emb_layer = layers.Embedding(30000, EMBED_DIM,input_length=max_length)(masked_sequence)\r\n",
        "  att_layer = layers.MultiHeadAttention(num_heads=8,key_dim=EMBED_DIM)(emb_layer,emb_layer,emb_layer)\r\n",
        "  drop_layer = layers.Dropout(0.1)(att_layer)\r\n",
        "  norm_layer = layers.LayerNormalization(epsilon = 1e-6)(emb_layer + drop_layer)\r\n",
        "  ff1_layer = layers.Dense(EMBED_DIM , activation='relu')(norm_layer)\r\n",
        "\r\n",
        "\r\n",
        "  #bu kisimda bir noktada activasyon fonksiyonu olmadan sadece embedding layeri kadar neurona sahip\r\n",
        "  #baska bir layer var onu da eklemem gerebilir\r\n",
        "  #AYRICA FF1_LAYER'IN CIKIS KISMI SHAPELERINE DIKKAT ET\r\n",
        "\r\n",
        "  return (ff1_layer)\r\n",
        "\r\n",
        "\r\n",
        "decision_model = tf.keras.Sequential([\r\n",
        "                                      \r\n",
        "                                      tf.keras.layers.Dense(10,activation = 'softmax'),\r\n",
        "                                      tf.keras.layers.Dense(1)\r\n",
        "])\r\n",
        "\r\n",
        "decision_model.compile(\r\n",
        "    loss= 'categorical_crossentropy',\r\n",
        "    optimizer =  'Adam',\r\n",
        "    metrics = ['accuracy']\r\n",
        ")\r\n",
        "decision_input = encoder_structure(masked_sequence)\r\n",
        "decision_model.fit(decision_input, masked_sequence, epochs=3)\r\n",
        "\r\n",
        "# BU NOKTADAN SONRA SOFTMAX FONKSIYONU KULLANILARAK OLUSTURULAN MODEL TRAIN EDILECEK \r\n",
        "# INPUT OLARAK ENCODER_STRUCTURE FONKIYONUN \r\n",
        "\r\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "1/1 [==============================] - 1s 531ms/step - loss: 190.7881 - accuracy: 0.3250\n",
            "Epoch 2/3\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 136.6859 - accuracy: 0.3250\n",
            "Epoch 3/3\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 130.5456 - accuracy: 0.3250\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f2301900240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rsFrO0ICT0A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "479dd55f-9b7e-4f95-f4c3-539b2ed86e95"
      },
      "source": [
        "emb_layer = layers.Embedding(30000, EMBED_DIM,input_length=max_length)(masked_sequence)\r\n",
        "att_layer = layers.MultiHeadAttention(num_heads=8,key_dim=EMBED_DIM)(emb_layer,emb_layer,emb_layer)\r\n",
        "drop_layer = layers.Dropout(0.1)(att_layer)\r\n",
        "norm_layer = layers.LayerNormalization(epsilon = 1e-6)(emb_layer + drop_layer)\r\n",
        "ff1_layer = layers.Dense(EMBED_DIM , activation='relu')(norm_layer)\r\n",
        "\r\n",
        "\r\n",
        "#### FF NN\r\n",
        "\r\n",
        "ff_model = tf.keras.Sequential([\r\n",
        "                                tf.keras.layers.Dense(EMBED_DIM, activation= 'relu'),\r\n",
        "                                tf.keras.layers.Dense(1)\r\n",
        "\r\n",
        "])\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# ff_model.compile(\r\n",
        "#     loss = 'categorical_crossentropy',\r\n",
        "#     optimizer = 'Adam',\r\n",
        "#     metrics = ['accuracy']\r\n",
        "# )\r\n",
        "\r\n",
        "# ff_model.fit(norm_layer, masked_sequence, epochs =50)\r\n",
        "print('norm_layer_shape',norm_layer.shape)\r\n",
        "print('masked_sequence_shape',masked_sequence.shape)\r\n",
        "print('encoder_structure_out_shape',ff1_layer.shape)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "norm_layer_shape (4, 10, 128)\n",
            "masked_sequence_shape (4, 10)\n",
            "encoder_structure_out_shape (4, 10, 128)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnHvFldIekIN"
      },
      "source": [
        ""
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5YM9XEJekEV"
      },
      "source": [
        ""
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeQz5O2-GRkB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "outputId": "78f7d794-6f97-442b-9cbd-2d6fd5c30efe"
      },
      "source": [
        "print('dense_layer_out', dense_1.shape)\r\n",
        "print('attention_layer_out',attention_layer.shape)\r\n",
        "print('softmax_layer_out',softmax_layer.shape)\r\n",
        "print('reduction_layer_out', reduction_layer.shape)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-0dca9a838d00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dense_layer_out'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'attention_layer_out'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattention_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'softmax_layer_out'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msoftmax_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'reduction_layer_out'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dense_1' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LTuqR2FGRgg"
      },
      "source": [
        "plt.plot(dense_1[0][1])\r\n",
        "plt.plot(attention_layer[0][1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVgpo3toCVaC"
      },
      "source": [
        "# input_layer = layers.Input(shape = (10,))\r\n",
        "# l1_layer = layers.Embedding(corpus_length, 10, input_length =10)(input_layer)\r\n",
        "# # l1_layer = layers.Flatten()(l1_layer)\r\n",
        "# out_layer  = layers.Dense(10, activation= 'softmax')(l1_layer)\r\n",
        "# tf_trial_model  = keras.Model(inputs= input_layer, outputs = out_layer)\r\n",
        "\r\n",
        "# tf_trial_model.compile(\r\n",
        "#     loss = 'categorical_crossentropy',\r\n",
        "#     optimizer = 'Adam',\r\n",
        "#     metrics = ['accuracy']\r\n",
        "# )\r\n",
        "\r\n",
        "# tf_trial_model.fit(\r\n",
        "#                   #  np.transpose(masked_sequence),\r\n",
        "#                    masked_sequence, \r\n",
        "#                    original_sequences, \r\n",
        "#                    epochs = 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNaxcn8HC49D"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USs96A4lIL_f"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2wgImRddC6k"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAB0auYbkBL_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jU_8aAyEuzVw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXQVRGMqLoVX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qN-jvQNIQE6c"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkn2S9-YyacX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}