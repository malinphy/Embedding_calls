{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_MLM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOjlT7fb7zlfvGkYahDg6qy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/malinphy/Embedding_calls/blob/main/BERT_MLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohty4PVDUm8m"
      },
      "source": [
        "import numpy as np \r\n",
        "import pandas as pd\r\n",
        "import random \r\n",
        "import re\r\n",
        "import tensorflow as tf \r\n",
        "from tensorflow import keras \r\n",
        "from tensorflow.keras import layers\r\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
        "\r\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\r\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
        "\r\n",
        "\r\n",
        "from tensorflow.keras.layers import Dense, Embedding, Flatten \r\n",
        "# MultiHeadAttention"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYyI72eIUn9I"
      },
      "source": [
        "## 1) Sentences will be tokenized using the TextVectorization \r\n",
        "## 2) 12.5 % of the tokens will be replaced with the [MASK] token. Substituded tokens will be \r\n",
        "## chosen randomly. To select the random tokens, random sampling will be chosen \r\n",
        "\r\n",
        "## 3) segment id will be determined at the second step\r\n",
        "## 4) positional embeddings will be directly taken from the article"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T95t9xM_UpXD"
      },
      "source": [
        "max_length =  128  ## 256\r\n",
        "corpus_length = 128   ## 30000\r\n",
        "embedding_dimension = 128   ## 128\r\n",
        "vocab_size = 128  ## 30000\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZXhaqbCUtkC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66597167-db4a-4261-b877-24b5a91504ba"
      },
      "source": [
        "trial_sentences = [\r\n",
        "    'my name is john',\r\n",
        "    'london is the capital is the england',\r\n",
        "    'weather is cold at the northern hemisphere of the world',\r\n",
        "    'today, i will go the school'\r\n",
        "]\r\n",
        "\r\n",
        "longest_sentence = []\r\n",
        "for i in trial_sentences:\r\n",
        "    longest_sentence.append(len(i.split()))\r\n",
        "    \r\n",
        "print(np.max(longest_sentence))\r\n",
        "\r\n",
        "longest_sentence = np.max(longest_sentence)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05Hg842CUvEZ"
      },
      "source": [
        "def masking_procedure (text):\r\n",
        "    \r\n",
        "    tf_tokenizer = Tokenizer()\r\n",
        "    tf_tokens=tf_tokenizer.fit_on_texts(text)\r\n",
        "\r\n",
        "    corpus = tf_tokenizer.word_index  ### TOKEN SET WITH TOKEN IDS WITHOUT THE SPECIAL TOKENS \r\n",
        "    extended_corpus = corpus.copy()   ### TOKEN SET WITH SPECIAL TOKENS\r\n",
        "\r\n",
        "    extended_corpus['MASK'] = len(corpus)+1\r\n",
        "\r\n",
        "    corpus_tokens = []   #### TOKEN SET WITHOUT THE SPECIAL TOKENS\r\n",
        "    for i,j in enumerate(corpus):\r\n",
        "        corpus_tokens.append(j)\r\n",
        "#         print(i,j)\r\n",
        "\r\n",
        "        \r\n",
        "    extended_tokens = []\r\n",
        "    for i,j in  enumerate(extended_corpus):\r\n",
        "    #     print(i,j)\r\n",
        "        extended_tokens.append(j)\r\n",
        "    # print('maskelenmis_corpus')\r\n",
        "    # print('-------')\r\n",
        "    subs_tokens=random.sample(corpus_tokens,int(len(corpus_tokens)*0.12)) ## RANDOMLY SELECTED TOKENS TO BE MASKED\r\n",
        "#     print(subs_tokens)\r\n",
        "\r\n",
        "    sequences = tf_tokenizer.texts_to_sequences(text)\r\n",
        "    padded_sequences= pad_sequences(sequences)\r\n",
        "    padded_sequences\r\n",
        "\r\n",
        "    substitute_padded_sequences= padded_sequences.copy()\r\n",
        "\r\n",
        "    subs_tokens_ids = [ ]  ## ID OF THE SUBS TOKENS\r\n",
        "\r\n",
        "    for i in subs_tokens:\r\n",
        "        subs_tokens_ids.append(tf_tokenizer.word_index[i])\r\n",
        "    \r\n",
        "    \r\n",
        "    mask_token_id = 20\r\n",
        "#     print(subs_tokens_ids)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "    for i in range(len(extended_tokens)):\r\n",
        "        if i == (subs_tokens_ids[0]-1   ):\r\n",
        "            extended_tokens[i] = '[MASK]'\r\n",
        "    \r\n",
        "        if i == (subs_tokens_ids[1]-1):\r\n",
        "            extended_tokens[i] = '[MASK]'\r\n",
        "        \r\n",
        "        \r\n",
        "        \r\n",
        "    maskeli = []\r\n",
        "    for i in range(substitute_padded_sequences.shape[0]):\r\n",
        "        for j in range(substitute_padded_sequences.shape[1]):\r\n",
        "            if substitute_padded_sequences[i][j] != 0:\r\n",
        "                x = (substitute_padded_sequences[i][j]  )\r\n",
        "                maskeli.append(\"\".join(extended_tokens[x-1]))\r\n",
        "                \r\n",
        "                \r\n",
        "    maskeli_2 = []\r\n",
        "    for i in (substitute_padded_sequences):\r\n",
        "        maskeli_2.append('\\n')\r\n",
        "        for j in range(len(i)):\r\n",
        "        \r\n",
        "            if (i[j] !=0):\r\n",
        "#             print(i[j])\r\n",
        "                maskeli_2.append(\"\".join(extended_tokens[i[j] -1]))\r\n",
        "    \r\n",
        "    \r\n",
        "    y= \" \".join(maskeli_2)\r\n",
        "\r\n",
        "    y = y.split('\\n')\r\n",
        "    y = np.delete(y,0,0)\r\n",
        "    y\r\n",
        "    \r\n",
        "    segment_ids = substitute_padded_sequences.copy()\r\n",
        "\r\n",
        "    for i in range(segment_ids.shape[0]):\r\n",
        "        for j in range(segment_ids.shape[1]):\r\n",
        "            if (segment_ids[i][j] != 0):\r\n",
        "                segment_ids[i][j] = 1\r\n",
        "    segment_ids = np.array(segment_ids)\r\n",
        "    \r\n",
        "    return (y , extended_tokens, segment_ids,substitute_padded_sequences)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NtDnwk6Uwei"
      },
      "source": [
        "masked_text, masked_tokens,segment_ids , masked_sequence = masking_procedure(trial_sentences)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51p_DKXsUxvm"
      },
      "source": [
        "MAX_LEN = 256\r\n",
        "EMBED_DIM = 128\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def get_pos_encoding_matrix(max_len, d_emb):\r\n",
        "    pos_enc = np.array(\r\n",
        "        [\r\n",
        "            [pos / np.power(10000, 2 * (j // 2) / d_emb) for j in range(d_emb)]\r\n",
        "            if pos != 0\r\n",
        "            else np.zeros(d_emb)\r\n",
        "            for pos in range(max_len)\r\n",
        "        ]\r\n",
        "    )\r\n",
        "    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2])  # dim 2i\r\n",
        "    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2])  # dim 2i+1\r\n",
        "    return pos_enc\r\n",
        "\r\n",
        "\r\n",
        "positional_embeddings = get_pos_encoding_matrix(MAX_LEN, EMBED_DIM)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKuES0kuUz-T"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}