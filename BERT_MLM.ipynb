{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_MLM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNIRsHhuHjj1kA70FwShthy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/malinphy/Embedding_calls/blob/main/BERT_MLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohty4PVDUm8m"
      },
      "source": [
        "import numpy as np \r\n",
        "import pandas as pd\r\n",
        "import random \r\n",
        "import re\r\n",
        "import tensorflow as tf \r\n",
        "from tensorflow import keras \r\n",
        "from tensorflow.keras import layers\r\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
        "\r\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\r\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
        "\r\n",
        "\r\n",
        "from tensorflow.keras.layers import Dense, Embedding, Flatten ,MultiHeadAttention,Dropout\r\n",
        "# MultiHeadAttention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYyI72eIUn9I"
      },
      "source": [
        "## 1) Sentences will be tokenized using the TextVectorization \r\n",
        "## 2) 12.5 % of the tokens will be replaced with the [MASK] token. Substituded tokens will be \r\n",
        "## chosen randomly. To select the random tokens, random sampling will be chosen \r\n",
        "\r\n",
        "## 3) segment id will be determined at the second step\r\n",
        "## 4) positional embeddings will be directly taken from the article"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T95t9xM_UpXD"
      },
      "source": [
        "max_length =  128  ## 256\r\n",
        "corpus_length = 128   ## 30000\r\n",
        "embedding_dimension = 128   ## 128\r\n",
        "vocab_size = 128  ## 30000\r\n",
        "number_heads = 8\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZXhaqbCUtkC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69717848-0c6f-44ee-ab7d-761f28258f56"
      },
      "source": [
        "trial_sentences = [\r\n",
        "    'my name is john',\r\n",
        "    'london is the capital is the england',\r\n",
        "    'weather is cold at the northern hemisphere of the world',\r\n",
        "    'today, i will go the school'\r\n",
        "]\r\n",
        "\r\n",
        "longest_sentence = []\r\n",
        "for i in trial_sentences:\r\n",
        "    longest_sentence.append(len(i.split()))\r\n",
        "    \r\n",
        "print(np.max(longest_sentence))\r\n",
        "\r\n",
        "longest_sentence = np.max(longest_sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05Hg842CUvEZ"
      },
      "source": [
        "def masking_procedure (text,tok):\r\n",
        "    \r\n",
        "    # tf_tokenizer = Tokenizer()\r\n",
        "    tf_tokenizer = tok\r\n",
        "    tf_tokens=tf_tokenizer.fit_on_texts(text)\r\n",
        "\r\n",
        "    corpus = tf_tokenizer.word_index  ### TOKEN SET WITH TOKEN IDS WITHOUT THE SPECIAL TOKENS \r\n",
        "    extended_corpus = corpus.copy()   ### TOKEN SET WITH SPECIAL TOKENS\r\n",
        "\r\n",
        "    extended_corpus['MASK'] = len(corpus)+1\r\n",
        "\r\n",
        "    corpus_tokens = []   #### TOKEN SET WITHOUT THE SPECIAL TOKENS\r\n",
        "    for i,j in enumerate(corpus):\r\n",
        "        corpus_tokens.append(j)\r\n",
        "#         print(i,j)\r\n",
        "\r\n",
        "        \r\n",
        "    extended_tokens = []\r\n",
        "    for i,j in  enumerate(extended_corpus):\r\n",
        "    #     print(i,j)\r\n",
        "        extended_tokens.append(j)\r\n",
        "    # print('maskelenmis_corpus')\r\n",
        "    # print('-------')\r\n",
        "    subs_tokens=random.sample(corpus_tokens,int(len(corpus_tokens)*0.12)) ## RANDOMLY SELECTED TOKENS TO BE MASKED\r\n",
        "#     print(subs_tokens)\r\n",
        "\r\n",
        "    sequences = tf_tokenizer.texts_to_sequences(text)\r\n",
        "    padded_sequences= pad_sequences(sequences)\r\n",
        "    padded_sequences\r\n",
        "\r\n",
        "    substitute_padded_sequences= padded_sequences.copy()\r\n",
        "    actual_padded_sequences = padded_sequences.copy()\r\n",
        "\r\n",
        "    subs_tokens_ids = [ ]  ## ID OF THE SUBS TOKENS\r\n",
        "\r\n",
        "    for i in subs_tokens:\r\n",
        "        subs_tokens_ids.append(tf_tokenizer.word_index[i])\r\n",
        "    \r\n",
        "    \r\n",
        "    mask_token_id = 20\r\n",
        "#     print(subs_tokens_ids)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "    for i in range(len(extended_tokens)):\r\n",
        "        if i == (subs_tokens_ids[0]-1   ):\r\n",
        "            extended_tokens[i] = '[MASK]'\r\n",
        "    \r\n",
        "        if i == (subs_tokens_ids[1]-1):\r\n",
        "            extended_tokens[i] = '[MASK]'\r\n",
        "        \r\n",
        "        \r\n",
        "        \r\n",
        "    maskeli = []\r\n",
        "    for i in range(substitute_padded_sequences.shape[0]):\r\n",
        "        for j in range(substitute_padded_sequences.shape[1]):\r\n",
        "            if substitute_padded_sequences[i][j] != 0:\r\n",
        "                x = (substitute_padded_sequences[i][j]  )\r\n",
        "                maskeli.append(\"\".join(extended_tokens[x-1]))\r\n",
        "                \r\n",
        "                \r\n",
        "    maskeli_2 = []\r\n",
        "    for i in (substitute_padded_sequences):\r\n",
        "        maskeli_2.append('\\n')\r\n",
        "        for j in range(len(i)):\r\n",
        "        \r\n",
        "            if (i[j] !=0):\r\n",
        "#             print(i[j])\r\n",
        "                maskeli_2.append(\"\".join(extended_tokens[i[j] -1]))\r\n",
        "    \r\n",
        "    \r\n",
        "    y= \" \".join(maskeli_2)\r\n",
        "\r\n",
        "    y = y.split('\\n')\r\n",
        "    y = np.delete(y,0,0)\r\n",
        "    y\r\n",
        "     \r\n",
        "    segment_ids = substitute_padded_sequences.copy()\r\n",
        "\r\n",
        "    for i in range(segment_ids.shape[0]):\r\n",
        "        for j in range(segment_ids.shape[1]):\r\n",
        "            if (segment_ids[i][j] != 0):\r\n",
        "                segment_ids[i][j] = 1\r\n",
        "    segment_ids = np.array(segment_ids)\r\n",
        "    \r\n",
        "    return (y , extended_tokens, segment_ids,substitute_padded_sequences,actual_padded_sequences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mteujAxNQsp"
      },
      "source": [
        "tf_tokenizer = Tokenizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NtDnwk6Uwei"
      },
      "source": [
        "masked_text, masked_tokens,segment_ids , masked_sequence, original_sequences = masking_procedure(trial_sentences,tf_tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWJiPeEbwwLP",
        "outputId": "86934097-733b-45d2-9112-5d129bec610d"
      },
      "source": [
        "masked_sequence,original_sequences,masked_text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[ 0,  0,  0,  0,  0,  0,  3,  4,  2,  5],\n",
              "        [ 0,  0,  0,  6,  2,  1,  7,  2,  1,  8],\n",
              "        [ 9,  2, 10, 11,  1, 12, 13, 14,  1, 15],\n",
              "        [ 0,  0,  0,  0, 16, 17, 18, 19,  1, 20]], dtype=int32),\n",
              " array([[ 0,  0,  0,  0,  0,  0,  3,  4,  2,  5],\n",
              "        [ 0,  0,  0,  6,  2,  1,  7,  2,  1,  8],\n",
              "        [ 9,  2, 10, 11,  1, 12, 13, 14,  1, 15],\n",
              "        [ 0,  0,  0,  0, 16, 17, 18, 19,  1, 20]], dtype=int32),\n",
              " array([' my [MASK] is john ', ' [MASK] is the capital is the england ',\n",
              "        ' weather is cold at the northern hemisphere of the world ',\n",
              "        ' today i will go the school'], dtype='<U57'))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51p_DKXsUxvm"
      },
      "source": [
        "MAX_LEN = 256\r\n",
        "EMBED_DIM = 128\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def get_pos_encoding_matrix(max_len, d_emb):\r\n",
        "    pos_enc = np.array(\r\n",
        "        [\r\n",
        "            [pos / np.power(10000, 2 * (j // 2) / d_emb) for j in range(d_emb)]\r\n",
        "            if pos != 0\r\n",
        "            else np.zeros(d_emb)\r\n",
        "            for pos in range(max_len)\r\n",
        "        ]\r\n",
        "    )\r\n",
        "    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2])  # dim 2i\r\n",
        "    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2])  # dim 2i+1\r\n",
        "    return pos_enc\r\n",
        "\r\n",
        "\r\n",
        "positional_embeddings = get_pos_encoding_matrix(MAX_LEN, EMBED_DIM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKuES0kuUz-T"
      },
      "source": [
        "pos_enc = get_pos_encoding_matrix(MAX_LEN, EMBED_DIM)\r\n",
        "# pos_enc + segment_ids\r\n",
        "\r\n",
        "total_encoding = masked_sequence+segment_ids # +pos_enc  (bi sekilde positional encodingi sokmam gerek )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEuqxovDCRpY"
      },
      "source": [
        "total_encoding.shape\r\n",
        "intro_shape = total_encoding.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rsFrO0ICT0A"
      },
      "source": [
        "# input_layer = layers.Input(shape = (10,))\r\n",
        "# emb_layer = layers.Embedding(intro_shape[0],embedding_dimension , input_length = max_length)(input_layer)\r\n",
        "# att_layer = layers.MultiHeadAttention(num_heads= number_heads , key_dim =embedding_dimension)(emb_layer,emb_layer,emb_layer)\r\n",
        "# dropout1_layer = layers.Dropout(0.1)(att_layer)\r\n",
        "# norm_layer_1 = layers.LayerNormalization(\r\n",
        "#         epsilon=1e-6\r\n",
        "#     )(emb_layer + dropout1_layer)\r\n",
        "# ### After this point feed forward nn will be constructed with dense layers\r\n",
        "\r\n",
        "# d1_layer = layers.Dense(128, activation = 'relu')(norm_layer_1)\r\n",
        "# # d2_layer = layers.Dense(embedding_dimension)\r\n",
        "\r\n",
        "# dropout2_layer = layers.Dropout(0.1)(d1_layer)\r\n",
        "\r\n",
        "# output_layer = layers.LayerNormalization(epsilon=1.06)(norm_layer_1 +d1_layer)\r\n",
        "\r\n",
        "\r\n",
        "# tf_mlm_model = keras.Model(inputs =input_layer, outputs = output_layer )\r\n",
        "# tf_mlm_model.summary()\r\n",
        "# keras.utils.plot_model(tf_mlm_model)\r\n",
        "\r\n",
        "\r\n",
        "# deneme_loss = tf.keras.losses.SparseCategoricalCrossentropy()\r\n",
        "\r\n",
        "\r\n",
        "# tf_mlm_model.compile(\r\n",
        "    \r\n",
        "#     loss = 'categorical_crossentropy',\r\n",
        "#     optimizer = 'Adam'\r\n",
        "# )\r\n",
        "\r\n",
        "# tf_mlm_model.fit(masked_sequence, original_sequences, epochs =5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVgpo3toCVaC"
      },
      "source": [
        "# input_layer = layers.Input(shape = (10,))\r\n",
        "# l1_layer = layers.Embedding(corpus_length, 10, input_length =10)(input_layer)\r\n",
        "# # l1_layer = layers.Flatten()(l1_layer)\r\n",
        "# out_layer  = layers.Dense(10, activation= 'softmax')(l1_layer)\r\n",
        "# tf_trial_model  = keras.Model(inputs= input_layer, outputs = out_layer)\r\n",
        "\r\n",
        "# tf_trial_model.compile(\r\n",
        "#     loss = 'categorical_crossentropy',\r\n",
        "#     optimizer = 'Adam',\r\n",
        "#     metrics = ['accuracy']\r\n",
        "# )\r\n",
        "\r\n",
        "# tf_trial_model.fit(\r\n",
        "#                   #  np.transpose(masked_sequence),\r\n",
        "#                    masked_sequence, \r\n",
        "#                    original_sequences, \r\n",
        "#                    epochs = 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNaxcn8HC49D"
      },
      "source": [
        "dink = layers.Embedding(300, 5)(masked_sequence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USs96A4lIL_f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3daa0e6-690b-4ca5-f851-88b1c92150e8"
      },
      "source": [
        "dink.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([4, 10, 5])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2wgImRddC6k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0933ae58-0163-40f3-dcb2-56075b180aa8"
      },
      "source": [
        "dink[1].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([10, 5])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAB0auYbkBL_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1a9ef94-dde5-434e-9172-36f89c6e9836"
      },
      "source": [
        "deneme_matrisi = np.array([[2],[4]])\r\n",
        "deneme_matrisi_2 = np.array([[1,2,3,4],\r\n",
        "                             [5,6,7,8]])\r\n",
        "deneme_matrisi.shape\r\n",
        "\r\n",
        "len(deneme_matrisi)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jU_8aAyEuzVw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42fa6b0f-c8f7-46cd-b4bc-3a62ecf65091"
      },
      "source": [
        "x = layers.Embedding(21,2,)(deneme_matrisi)\r\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[[-0.04596917 -0.04789708]]\n",
            "\n",
            " [[-0.03241487 -0.01382715]]], shape=(2, 1, 2), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXQVRGMqLoVX",
        "outputId": "a2806e91-f82f-4abe-a0c0-16ecf7ae1417"
      },
      "source": [
        "layers.Dense(1,activation = 'softmax')(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 1, 1), dtype=float32, numpy=\n",
              "array([[[1.]],\n",
              "\n",
              "       [[1.]]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qN-jvQNIQE6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75eae40c-116a-4003-cde2-5800c7aef417"
      },
      "source": [
        "layers.Embedding(corpus_length, 1, input_length =10)(masked_sequence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(4, 10, 1), dtype=float32, numpy=\n",
              "array([[[-0.03723358],\n",
              "        [-0.03723358],\n",
              "        [-0.03723358],\n",
              "        [-0.03723358],\n",
              "        [-0.03723358],\n",
              "        [-0.03723358],\n",
              "        [ 0.04986315],\n",
              "        [ 0.04745692],\n",
              "        [-0.01612135],\n",
              "        [-0.00112344]],\n",
              "\n",
              "       [[-0.03723358],\n",
              "        [-0.03723358],\n",
              "        [-0.03723358],\n",
              "        [-0.04053469],\n",
              "        [-0.01612135],\n",
              "        [-0.03321778],\n",
              "        [ 0.04913745],\n",
              "        [-0.01612135],\n",
              "        [-0.03321778],\n",
              "        [ 0.04846687]],\n",
              "\n",
              "       [[-0.00867294],\n",
              "        [-0.01612135],\n",
              "        [-0.02571096],\n",
              "        [-0.03978658],\n",
              "        [-0.03321778],\n",
              "        [-0.04569636],\n",
              "        [ 0.02915999],\n",
              "        [ 0.02300172],\n",
              "        [-0.03321778],\n",
              "        [-0.0275734 ]],\n",
              "\n",
              "       [[-0.03723358],\n",
              "        [-0.03723358],\n",
              "        [-0.03723358],\n",
              "        [-0.03723358],\n",
              "        [ 0.02888061],\n",
              "        [-0.0422257 ],\n",
              "        [-0.01365086],\n",
              "        [ 0.03446329],\n",
              "        [-0.03321778],\n",
              "        [-0.03740649]]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkn2S9-YyacX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}