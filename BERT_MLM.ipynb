{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_MLM.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPP6Nd0jmmNC3IDJ539jVtG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/malinphy/Embedding_calls/blob/main/BERT_MLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohty4PVDUm8m"
      },
      "source": [
        "import numpy as np \r\n",
        "import pandas as pd\r\n",
        "import random \r\n",
        "import re\r\n",
        "import tensorflow as tf \r\n",
        "from tensorflow import keras \r\n",
        "from tensorflow.keras import layers\r\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
        "\r\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\r\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYyI72eIUn9I"
      },
      "source": [
        "## 1) Sentences will be tokenized using the TextVectorization \r\n",
        "## 2) 12.5 % of the tokens will be replaced with the [MASK] token. Substituded tokens will be \r\n",
        "## chosen randomly. To select the random tokens, random sampling will be chosen \r\n",
        "\r\n",
        "## 3) segment id will be determined at the second step\r\n",
        "## 4) positional embeddings will be directly taken from the article"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T95t9xM_UpXD",
        "outputId": "ca6680f1-c7ee-4741-9a6b-0f11ee807227"
      },
      "source": [
        "trial_sentences = [\r\n",
        "    'my name is john',\r\n",
        "    'london is the capital is the england',\r\n",
        "    'weather is cold at the northern hemisphere of the world',\r\n",
        "    'today, i will go the school'\r\n",
        "]\r\n",
        "\r\n",
        "longest_sentence = []\r\n",
        "for i in trial_sentences:\r\n",
        "    longest_sentence.append(len(i.split()))\r\n",
        "    \r\n",
        "print(np.max(longest_sentence))\r\n",
        "\r\n",
        "longest_sentence = np.max(longest_sentence)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZXhaqbCUtkC"
      },
      "source": [
        "vectorizer = tf.keras.layers.experimental.preprocessing.TextVectorization(\r\n",
        "                            max_tokens = 30,\r\n",
        "#                             standardize=LOWER_AND_STRIP_PUNCTUATION,\r\n",
        "#                             split=SPLIT_ON_WHITESPACE\r\n",
        "                            output_sequence_length = int(longest_sentence)\r\n",
        ")\r\n",
        "\r\n",
        "vectorizer.adapt(trial_sentences)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05Hg842CUvEZ"
      },
      "source": [
        "tf_tokenizer = Tokenizer()\r\n",
        "tf_tokens=tf_tokenizer.fit_on_texts(trial_sentences)\r\n",
        "\r\n",
        "corpus = tf_tokenizer.word_index\r\n",
        "extended_corpus = corpus.copy()\r\n",
        "\r\n",
        "extended_corpus['MASK'] = len(corpus)+1\r\n",
        "\r\n",
        "corpus_tokens = []\r\n",
        "for i,j in enumerate(corpus):\r\n",
        "    corpus_tokens.append(j)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NtDnwk6Uwei",
        "outputId": "d21b4005-a091-4fbc-8a6d-53ea9e0c94e1"
      },
      "source": [
        "extended_tokens = []\r\n",
        "for i,j in  enumerate(extended_corpus):\r\n",
        "    print(i,j)\r\n",
        "    extended_tokens.append(j)\r\n",
        "    \r\n",
        "subs_tokens=random.sample(corpus_tokens,int(len(corpus_tokens)*0.12))\r\n",
        "print(subs_tokens)\r\n",
        "\r\n",
        "sequences = tf_tokenizer.texts_to_sequences(trial_sentences)\r\n",
        "padded_sequences= pad_sequences(sequences)\r\n",
        "padded_sequences\r\n",
        "\r\n",
        "substitute_padded_sequences= padded_sequences.copy()\r\n",
        "\r\n",
        "substitute_token_ids = [ ]\r\n",
        "\r\n",
        "for i in subs_tokens:\r\n",
        "    substitute_token_ids.append(tf_tokenizer.word_index[i])\r\n",
        "    \r\n",
        "    \r\n",
        "mask_token_id = 20\r\n",
        "print(substitute_token_ids)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 the\n",
            "1 is\n",
            "2 my\n",
            "3 name\n",
            "4 john\n",
            "5 london\n",
            "6 capital\n",
            "7 england\n",
            "8 weather\n",
            "9 cold\n",
            "10 at\n",
            "11 northern\n",
            "12 hemisphere\n",
            "13 of\n",
            "14 world\n",
            "15 today\n",
            "16 i\n",
            "17 will\n",
            "18 go\n",
            "19 school\n",
            "20 MASK\n",
            "['name', 'the']\n",
            "[4, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51p_DKXsUxvm",
        "outputId": "5d9da04c-1e78-4a2b-c649-05c169f2d34e"
      },
      "source": [
        "for i in range(len(extended_tokens)):\r\n",
        "    if i == (substitute_token_ids[0]-1 or substitute_token_ids[1]-1  ):\r\n",
        "        print(i)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKuES0kuUz-T"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}