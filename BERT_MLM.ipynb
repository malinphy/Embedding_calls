{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_MLM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMCAHdfQxMxbc6ZT4bXac96",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/malinphy/Embedding_calls/blob/main/BERT_MLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohty4PVDUm8m"
      },
      "source": [
        "import numpy as np \r\n",
        "import pandas as pd\r\n",
        "import random \r\n",
        "import re\r\n",
        "import tensorflow as tf \r\n",
        "from tensorflow import keras \r\n",
        "from tensorflow.keras import layers\r\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
        "\r\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\r\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
        "\r\n",
        "\r\n",
        "from tensorflow.keras.layers import Dense, Embedding, Flatten ,MultiHeadAttention,Dropout\r\n",
        "from tensorflow.keras import Sequential\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYyI72eIUn9I"
      },
      "source": [
        "## 1) Sentences will be tokenized using the TextVectorization \r\n",
        "## 2) 12.5 % of the tokens will be replaced with the [MASK] token. Substituded tokens will be \r\n",
        "## chosen randomly. To select the random tokens, random sampling will be chosen \r\n",
        "\r\n",
        "## 3) segment id will be determined at the second step\r\n",
        "## 4) positional embeddings will be directly taken from the article"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T95t9xM_UpXD"
      },
      "source": [
        "padding_length =  12  ## 128\r\n",
        "# corpus_length = 128   ## 30000\r\n",
        "embedding_dimension = 128   ## 128\r\n",
        "vocab_size = 128  ## 30000\r\n",
        "number_heads = 8\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZXhaqbCUtkC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b514c5f-b8e6-4511-c69b-d6c733232e9f"
      },
      "source": [
        "trial_sentences = [\r\n",
        "    'my name is john',\r\n",
        "    'london is the capital is the england',\r\n",
        "    'weather is cold at the northern hemisphere of the world',\r\n",
        "    'today, i will go the school',\r\n",
        "    # 'London is the at northern side of the world'\r\n",
        "]\r\n",
        "\r\n",
        "\r\n",
        "# prediction_sentence = 'my name is [MASK]'\r\n",
        "# prediction_tokens = tokenizing_procedure(prediction_sentence, tf_tokenizer)\r\n",
        "\r\n",
        "\r\n",
        "longest_sentence = []\r\n",
        "for i in trial_sentences:\r\n",
        "    longest_sentence.append(len(i.split()))\r\n",
        "    \r\n",
        "print(np.max(longest_sentence))\r\n",
        "\r\n",
        "longest_sentence = np.max(longest_sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZt9TQ1lQE20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe61ae2f-e04d-462a-9ccb-88e747518904"
      },
      "source": [
        "tf_tokenizer= Tokenizer()\r\n",
        "\r\n",
        "def tokenizing_procedure(text,tokenizer,padding_length): ##### Return the corpus and the padded_sequences\r\n",
        "  # this function works as I expected\r\n",
        "  tf_tokenizer = tokenizer\r\n",
        "  tf_tokens = tf_tokenizer.fit_on_texts(text)\r\n",
        "\r\n",
        "  corpus = tf_tokenizer.word_index\r\n",
        "  sequences = tf_tokenizer.texts_to_sequences(text)\r\n",
        "  padded_sequences = pad_sequences(sequences,maxlen = padding_length)\r\n",
        "  corpus['MASK'] = len(corpus)+1\r\n",
        "\r\n",
        "  return padded_sequences, corpus\r\n",
        "\r\n",
        "\r\n",
        "unmasked_sequence, corpus = tokenizing_procedure(trial_sentences, tf_tokenizer,padding_length)\r\n",
        "\r\n",
        "print(unmasked_sequence)\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0  0  0  0  0  0  0  0  3  4  2  5]\n",
            " [ 0  0  0  0  0  6  2  1  7  2  1  8]\n",
            " [ 0  0  9  2 10 11  1 12 13 14  1 15]\n",
            " [ 0  0  0  0  0  0 16 17 18 19  1 20]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moIZnWX4Qmtq",
        "outputId": "7a97f46e-0435-4f2b-ecc4-1a482ad1e255"
      },
      "source": [
        "def masking(corpus, padded_sequences,tokenizer):\r\n",
        "  masked_corpus = corpus.copy()\r\n",
        "  masked_corpus['[MASK]'] = len(corpus)+1\r\n",
        "  corpus_tokens = []   #### TOKEN SET WITHOUT THE SPECIAL TOKENS\r\n",
        "  for i,j in enumerate(masked_corpus):\r\n",
        "      corpus_tokens.append(j)\r\n",
        "\r\n",
        "  subs_tokens=random.sample(corpus_tokens,int(len(corpus_tokens)*0.12))\r\n",
        "  subs_tokens\r\n",
        "\r\n",
        "  subs_tokens_ids = [ ]  ## ID OF THE SUBS TOKENS\r\n",
        "\r\n",
        "  for i in subs_tokens:\r\n",
        "      subs_tokens_ids.append(tf_tokenizer.word_index[i])\r\n",
        "\r\n",
        "  mask_token_id = 20\r\n",
        "  masked_token_id = len(corpus)+1\r\n",
        "\r\n",
        "  for i in range(len(corpus_tokens)):\r\n",
        "        if i == (subs_tokens_ids[0]-1   ):\r\n",
        "            corpus_tokens[i] = '[MASK]'\r\n",
        "    \r\n",
        "        if i == (subs_tokens_ids[1]-1):\r\n",
        "            corpus_tokens[i] = '[MASK]'\r\n",
        "\r\n",
        "  for i in range(padded_sequences.shape[0]):\r\n",
        "        for j in range(padded_sequences.shape[1]):\r\n",
        "          for k in range(len(subs_tokens_ids)):\r\n",
        "            if padded_sequences[i][j] == (subs_tokens_ids[k]):\r\n",
        "\r\n",
        "                padded_sequences[i][j] = masked_token_id \r\n",
        "            if padded_sequences[i][j] == (subs_tokens_ids[k]):\r\n",
        "                \r\n",
        "                padded_sequences[i][j] = masked_token_id \r\n",
        "\r\n",
        "\r\n",
        "  return padded_sequences, masked_token_id\r\n",
        "        \r\n",
        "masked_padded_sequence, masked_token_id = masking(corpus, unmasked_sequence, tf_tokenizer)\r\n",
        "print('masked sequence','\\n',masked_padded_sequence)\r\n",
        "print('unmasked seqeunce','\\n',unmasked_sequence)\r\n",
        "print(masked_token_id)\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "masked sequence \n",
            " [[ 0  0  0  0  0  0  0  0  3  4  2  5]\n",
            " [ 0  0  0  0  0  6  2  1  7  2  1  8]\n",
            " [ 0  0 22  2 10 11  1 12 13 14  1 15]\n",
            " [ 0  0  0  0  0  0 16 17 18 19  1 22]]\n",
            "unmasked seqeunce \n",
            " [[ 0  0  0  0  0  0  0  0  3  4  2  5]\n",
            " [ 0  0  0  0  0  6  2  1  7  2  1  8]\n",
            " [ 0  0 22  2 10 11  1 12 13 14  1 15]\n",
            " [ 0  0  0  0  0  0 16 17 18 19  1 22]]\n",
            "22\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJG-K7QvQzXS"
      },
      "source": [
        "def segment_tokenizer(masked_padded_sequence):\r\n",
        "  segment_ids = masked_padded_sequence.copy()\r\n",
        "  for i in range(segment_ids.shape[0]):\r\n",
        "    for j in range(segment_ids.shape[1]):\r\n",
        "      if segment_ids[i][j] != 0:\r\n",
        "        segment_ids[i][j] = 1\r\n",
        "\r\n",
        "  return segment_ids\r\n",
        "\r\n",
        "segment_ids = segment_tokenizer(masked_padded_sequence)\r\n",
        "# def masked_text_maker(masked_padded_sequences, masked_token_id):\r\n",
        "\r\n",
        "#   for i in range(masked_padded_sequences.shape[0]):\r\n",
        "#         for j in range(masked_padded_sequences.shape[1]):\r\n",
        "#             if masked_padded_sequences[i][j] != 0:\r\n",
        "#                 x = (masked_padded_sequences[i][j]  )\r\n",
        "#                 maskeli.append(\"\".join(extended_tokens[x-1]))\r\n",
        "                \r\n",
        "                \r\n",
        "#   maskeli_2 = []\r\n",
        "#   for i in (masked_padded_sequences):\r\n",
        "#       maskeli_2.append('\\n')\r\n",
        "#       for j in range(len(i)):\r\n",
        "        \r\n",
        "#           if (i[j] !=0):\r\n",
        "# #            print(i[j])\r\n",
        "#               maskeli_2.append(\"\".join(extended_tokens[i[j] -1]))\r\n",
        "    \r\n",
        "    \r\n",
        "#   y= \" \".join(maskeli_2)\r\n",
        "\r\n",
        "#   y = y.split('\\n')\r\n",
        "#   y = np.delete(y,0,0)\r\n",
        "  \r\n",
        "#   return y\r\n",
        "\r\n",
        "# masked_text_maker(masked_padded_sequence, masked_token_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6b3sGLKzfX6",
        "outputId": "898eab84-73d0-4c51-c2b6-ea5ff892a662"
      },
      "source": [
        "total_tokens = segment_ids + masked_padded_sequence\r\n",
        "\r\n",
        "total_tokens.shape\r\n",
        "print(unmasked_sequence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0  0  0  0  0  0  0  0  3  4  2  5]\n",
            " [ 0  0  0  0  0  6  2  1  7  2  1  8]\n",
            " [ 0  0 22  2 10 11  1 12 13 14  1 15]\n",
            " [ 0  0  0  0  0  0 16 17 18 19  1 22]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05Hg842CUvEZ"
      },
      "source": [
        "# def masking_procedure (text,tok):\r\n",
        "    \r\n",
        "#     # tf_tokenizer = Tokenizer()\r\n",
        "#     tf_tokenizer = tok\r\n",
        "#     tf_tokens=tf_tokenizer.fit_on_texts(text)\r\n",
        "\r\n",
        "#     corpus = tf_tokenizer.word_index  ### TOKEN SET WITH TOKEN IDS WITHOUT THE SPECIAL TOKENS \r\n",
        "#     extended_corpus = corpus.copy()   ### TOKEN SET WITH SPECIAL TOKENS\r\n",
        "\r\n",
        "#     extended_corpus['MASK'] = len(corpus)+1\r\n",
        "\r\n",
        "#     corpus_tokens = []   #### TOKEN SET WITHOUT THE SPECIAL TOKENS\r\n",
        "#     for i,j in enumerate(corpus):\r\n",
        "#         corpus_tokens.append(j)\r\n",
        "# #         print(i,j)\r\n",
        "\r\n",
        "        \r\n",
        "#     extended_tokens = []\r\n",
        "#     for i,j in  enumerate(extended_corpus):\r\n",
        "#     #     print(i,j)\r\n",
        "#         extended_tokens.append(j)\r\n",
        "#     # print('maskelenmis_corpus')\r\n",
        "#     # print('-------')\r\n",
        "#     subs_tokens=random.sample(corpus_tokens,int(len(corpus_tokens)*0.12)) ## RANDOMLY SELECTED TOKENS TO BE MASKED\r\n",
        "# #     print(subs_tokens)\r\n",
        "\r\n",
        "#     sequences = tf_tokenizer.texts_to_sequences(text)\r\n",
        "#     padded_sequences= pad_sequences(sequences)\r\n",
        "#     padded_sequences\r\n",
        "\r\n",
        "#     substitute_padded_sequences= padded_sequences.copy()\r\n",
        "#     actual_padded_sequences = padded_sequences.copy()\r\n",
        "\r\n",
        "#     subs_tokens_ids = [ ]  ## ID OF THE SUBS TOKENS\r\n",
        "\r\n",
        "#     for i in subs_tokens:\r\n",
        "#         subs_tokens_ids.append(tf_tokenizer.word_index[i])\r\n",
        "    \r\n",
        "    \r\n",
        "#     mask_token_id = 20\r\n",
        "# #     print(subs_tokens_ids)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "#     for i in range(len(extended_tokens)):\r\n",
        "#         if i == (subs_tokens_ids[0]-1   ):\r\n",
        "#             extended_tokens[i] = '[MASK]'\r\n",
        "    \r\n",
        "#         if i == (subs_tokens_ids[1]-1):\r\n",
        "#             extended_tokens[i] = '[MASK]'\r\n",
        "        \r\n",
        "        \r\n",
        "        \r\n",
        "#     maskeli = []\r\n",
        "#     for i in range(substitute_padded_sequences.shape[0]):\r\n",
        "#         for j in range(substitute_padded_sequences.shape[1]):\r\n",
        "#             if substitute_padded_sequences[i][j] != 0:\r\n",
        "#                 x = (substitute_padded_sequences[i][j]  )\r\n",
        "#                 maskeli.append(\"\".join(extended_tokens[x-1]))\r\n",
        "                \r\n",
        "                \r\n",
        "#     maskeli_2 = []\r\n",
        "#     for i in (substitute_padded_sequences):\r\n",
        "#         maskeli_2.append('\\n')\r\n",
        "#         for j in range(len(i)):\r\n",
        "        \r\n",
        "#             if (i[j] !=0):\r\n",
        "# #             print(i[j])\r\n",
        "#                 maskeli_2.append(\"\".join(extended_tokens[i[j] -1]))\r\n",
        "    \r\n",
        "    \r\n",
        "#     y= \" \".join(maskeli_2)\r\n",
        "\r\n",
        "#     y = y.split('\\n')\r\n",
        "#     y = np.delete(y,0,0)\r\n",
        "#     y\r\n",
        "     \r\n",
        "#     segment_ids = substitute_padded_sequences.copy()\r\n",
        "\r\n",
        "#     for i in range(segment_ids.shape[0]):\r\n",
        "#         for j in range(segment_ids.shape[1]):\r\n",
        "#             if (segment_ids[i][j] != 0):\r\n",
        "#                 segment_ids[i][j] = 1\r\n",
        "#     segment_ids = np.array(segment_ids)\r\n",
        "    \r\n",
        "#     return (y , extended_tokens, segment_ids,substitute_padded_sequences,actual_padded_sequences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51p_DKXsUxvm"
      },
      "source": [
        "MAX_LEN = 256\r\n",
        "EMBED_DIM = 128\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def get_pos_encoding_matrix(max_len, d_emb):\r\n",
        "    pos_enc = np.array(\r\n",
        "        [\r\n",
        "            [pos / np.power(10000, 2 * (j // 2) / d_emb) for j in range(d_emb)]\r\n",
        "            if pos != 0\r\n",
        "            else np.zeros(d_emb)\r\n",
        "            for pos in range(max_len)\r\n",
        "        ]\r\n",
        "    )\r\n",
        "    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2])  # dim 2i\r\n",
        "    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2])  # dim 2i+1\r\n",
        "    return pos_enc\r\n",
        "\r\n",
        "\r\n",
        "positional_embeddings = get_pos_encoding_matrix(MAX_LEN, EMBED_DIM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKuES0kuUz-T"
      },
      "source": [
        "# pos_enc = get_pos_encoding_matrix(MAX_LEN, EMBED_DIM)\r\n",
        "# # pos_enc + segment_ids\r\n",
        "\r\n",
        "# total_encoding = masked_sequence+segment_ids # +pos_enc  (bi sekilde positional encodingi sokmam gerek )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnHvFldIekIN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7505638-7e40-4c7a-ab10-91efad9a721b"
      },
      "source": [
        "total_tokens #  INPUT SEQUENCE \r\n",
        "unmasked_sequence  # LOGITS\r\n",
        "total_tokens.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 12)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4oAB_qJ-wnWU",
        "outputId": "6bcedae2-fac6-45a0-fe05-774969b254c6"
      },
      "source": [
        "## bu kismi embedding layer'i ekleyecegim \r\n",
        "def vectorizer(X,vocab_size, embedding_dimension):\r\n",
        "  embedding_out =  layers.Embedding(vocab_size, embedding_dimension)(X)\r\n",
        "\r\n",
        "  return (embedding_out)\r\n",
        "\r\n",
        "x_input_vectors = vectorizer(total_tokens,1000,embedding_dimension) #### MLM INPUT X\r\n",
        "print('total_vectors_shape', x_input_vectors.shape)\r\n",
        "y_input_vectors = vectorizer(unmasked_sequence, 1000,embedding_dimension) #### MLM INPUT Y\r\n",
        "print('total_sequences_shape', y_input_vectors.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total_vectors_shape (4, 12, 128)\n",
            "total_sequences_shape (4, 12, 128)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPiq1DF2Cy2a",
        "outputId": "afa5f881-36fe-4b70-82e8-cd4fd5cec453"
      },
      "source": [
        "y = layers.Embedding(\r\n",
        "        vocab_size, embedding_dimension, name=\"word_embedding\"\r\n",
        "    )(unmasked_sequence)\r\n",
        "\r\n",
        "y.shape\r\n",
        "\r\n",
        "unmasked_sequence.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 12)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmhYpu7JAiZN",
        "outputId": "917c8a85-4233-46c7-f891-7ced4e995028"
      },
      "source": [
        "inputs = layers.Input((12), dtype=tf.int64)\r\n",
        "\r\n",
        "word_embeddings = layers.Embedding(\r\n",
        "        vocab_size, embedding_dimension, name=\"word_embedding\"\r\n",
        "    )(inputs)\r\n",
        "# position_embeddings = layers.Embedding(\r\n",
        "#         input_dim=padding_length,\r\n",
        "#         output_dim=config.EMBED_DIM,\r\n",
        "#         # weights=[get_pos_encoding_matrix(padding_length, config.EMBED_DIM)],\r\n",
        "#         name=\"position_embedding\",\r\n",
        "#     )(tf.range(start=0, limit=config.MAX_LEN, delta=1))\r\n",
        "embeddings = word_embeddings# + position_embeddings\r\n",
        "\r\n",
        "encoder_output = embeddings\r\n",
        "    \r\n",
        "for i in range(1):\r\n",
        "    attention_output = layers.MultiHeadAttention(\r\n",
        "    num_heads=8,\r\n",
        "    key_dim=embedding_dimension // 8)(encoder_output, encoder_output, encoder_output)\r\n",
        "    attention_output = layers.Dropout(0.1)(attention_output)\r\n",
        "    attention_output = layers.LayerNormalization(\r\n",
        "    epsilon=1e-6)(encoder_output + attention_output)\r\n",
        "\r\n",
        "    # Feed-forward layer\r\n",
        "ffn = keras.Sequential(\r\n",
        "        [\r\n",
        "            layers.Dense(128, activation=\"relu\"),\r\n",
        "            layers.Dense(embedding_dimension),\r\n",
        "        ]\r\n",
        "        )\r\n",
        "ffn_output = ffn(attention_output)\r\n",
        "ffn_output = layers.Dropout(0.1)(ffn_output)\r\n",
        "sequence_output = layers.LayerNormalization(\r\n",
        "        epsilon=1e-6)(attention_output + ffn_output)\r\n",
        "        # encoder_output = bert_module(encoder_output, encoder_output, encoder_output, i)\r\n",
        "\r\n",
        "mlm_output = layers.Dense(vocab_size,  activation=\"softmax\")(sequence_output)\r\n",
        "mlm_model = keras.Model(inputs, mlm_output, name=\"masked_bert_model\")\r\n",
        "mlm_model.compile(\r\n",
        "    loss = 'categorical_crossentropy',\r\n",
        "    optimizer = 'Adam'\r\n",
        ")\r\n",
        "# optimizer = keras.optimizers.Adam(learning_rate=config.LR)\r\n",
        "# mlm_model.compile(optimizer=optimizer)\r\n",
        "    # return mlm_model\r\n",
        "mlm_model.fit(unmasked_sequence, y, epochs=5, \r\n",
        "              # callbacks=[generator_callback]\r\n",
        "              )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1/1 [==============================] - 1s 1s/step - loss: -0.8590\n",
            "Epoch 2/5\n",
            "1/1 [==============================] - 0s 11ms/step - loss: -1.3417\n",
            "Epoch 3/5\n",
            "1/1 [==============================] - 0s 11ms/step - loss: -1.7703\n",
            "Epoch 4/5\n",
            "1/1 [==============================] - 0s 9ms/step - loss: -2.1667\n",
            "Epoch 5/5\n",
            "1/1 [==============================] - 0s 12ms/step - loss: -2.4986\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f021b9b8650>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3KPwfMVMZPp",
        "outputId": "34954ab9-eb43-4365-e1b3-d2fe11b17a75"
      },
      "source": [
        "masked_padded_sequence.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 12)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igtKQYiU1yXM"
      },
      "source": [
        "\r\n",
        "\r\n",
        "\r\n",
        "# total_input = keras.Input( (12,128),dtype=tf.int64)\r\n",
        "\r\n",
        "# att_layer = layers.MultiHeadAttention(num_heads =8, key_dim = embedding_dimension)(x_input_vectors,\r\n",
        "#                                                                                    x_input_vectors,\r\n",
        "#                                                                                    x_input_vectors)\r\n",
        "# attention_output = layers.Dropout(0.1)(att_layer)\r\n",
        "# norm_layer = layers.LayerNormalization(epsilon = 1e-6)(attention_output+x_input_vectors)\r\n",
        "\r\n",
        "# ff_net = tf.keras.Sequential([\r\n",
        "#                               layers.Dense(128, activation= 'relu'),\r\n",
        "#                               layers.Dense(embedding_dimension)\r\n",
        "\r\n",
        "# ])\r\n",
        "# ff_net_out = ff_net(norm_layer)\r\n",
        "# ff_net_out  = layers.Dropout(0.1)(ff_net_out)\r\n",
        "\r\n",
        "# sequence_output = layers.LayerNormalization(epsilon = 1e-6)(attention_output+ff_net_out)\r\n",
        "\r\n",
        "# mlm_out = layers.Dense(vocab_size,  activation=\"softmax\")(sequence_output)\r\n",
        "# fin = layers.Dense(vocab_size)(mlm_out)\r\n",
        "# mlm_model = keras.Model(inputs = total_input, outputs = sequence_output)\r\n",
        "# mlm_model.compile(\r\n",
        "#     loss = 'categorical_crossentrop',\r\n",
        "#     optimizer = 'Adam',\r\n",
        "#     metrics = ['accuracy']\r\n",
        "\r\n",
        "# )\r\n",
        "# mlm_model.fit(total_vectors, total_seqeunces)                 \r\n",
        "\r\n",
        "# print(mlm_out.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEIrE_mj2qdC"
      },
      "source": [
        "sequence_output.shape\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngusS3rL2qzl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3S6UmHXpzn2t"
      },
      "source": [
        "input_layer1 = keras.Input(shape = (padding_length,128))\r\n",
        "att_layer1 = layers.MultiHeadAttention(num_heads = 8, key_dim =embedding_dimension)(input_layer1,\r\n",
        "                                                                         input_layer1,\r\n",
        "                                                                         input_layer1)\r\n",
        "drop_layer1 = layers.Dropout(0.1)(att_layer1)\r\n",
        "norm_layer1 = layers.LayerNormalization(epsilon = 1e-6)(input_layer1 + drop_layer1)\r\n",
        "ff1_layer1 = layers.Dense(embedding_dimension , activation='relu')(norm_layer1)\r\n",
        "ff2_layer1 = layers.Dense(embedding_dimension)\r\n",
        "drop2_layer1 = layers.Dropout(0.1)(ff1_layer1)\r\n",
        "norm2_layer1 = layers.LayerNormalization(epsilon = 1e-6)(input_layer1 + ff2_layer1)\r\n",
        "# output_layer1 = layers.Dense(128,activation = 'softmax')(att_layer1+ff2_layer1)\r\n",
        "# small_model1 = keras.Model(input_layer1, output_layer1)\r\n",
        "\r\n",
        "# small_model1.compile(\r\n",
        "#     loss = 'categorical_crossentropy',\r\n",
        "#     optimizer = 'Adam',\r\n",
        "#     metrics = ['accuracy']\r\n",
        "# )\r\n",
        "\r\n",
        "# # small_model1.summary()\r\n",
        "# small_model1.fit(total_vectors, total_sequences, epochs=100)\r\n",
        "\r\n",
        "# ff2_layer1\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7_JPOaFQL7r"
      },
      "source": [
        "sample_token =  ['London is the at northern side of the [MASK]']\r\n",
        "\r\n",
        "\r\n",
        "# tf_tokenizer= Tokenizer()\r\n",
        "\r\n",
        "tokenized_sample , corpus = tokenizing_procedure(sample_token,tf_tokenizer)\r\n",
        "print(tokenized_sample)\r\n",
        "pad_sequences(tokenized_sample,maxlen = 10)\r\n",
        "# masking(corpus, tokenized_sample,tf_tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5YM9XEJekEV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LTuqR2FGRgg"
      },
      "source": [
        "prediction_sentence = 'my name is MASK'\r\n",
        "tokenizing_procedure(prediction_sentence,tf_tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeQz5O2-GRkB"
      },
      "source": [
        "\r\n",
        "\r\n",
        "prediction_tokens, ikincil = tokenizing_procedure(prediction_sentence, tf_tokenizer)\r\n",
        "np.argmax(small_model.predict(prediction_tokens))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVgpo3toCVaC"
      },
      "source": [
        "# input_layer = layers.Input(shape = (10,))\r\n",
        "# l1_layer = layers.Embedding(corpus_length, 10, input_length =10)(input_layer)\r\n",
        "# # l1_layer = layers.Flatten()(l1_layer)\r\n",
        "# out_layer  = layers.Dense(10, activation= 'softmax')(l1_layer)\r\n",
        "# tf_trial_model  = keras.Model(inputs= input_layer, outputs = out_layer)\r\n",
        "\r\n",
        "# tf_trial_model.compile(\r\n",
        "#     loss = 'categorical_crossentropy',\r\n",
        "#     optimizer = 'Adam',\r\n",
        "#     metrics = ['accuracy']\r\n",
        "# )\r\n",
        "\r\n",
        "# tf_trial_model.fit(\r\n",
        "#                   #  np.transpose(masked_sequence),\r\n",
        "#                    masked_sequence, \r\n",
        "#                    original_sequences, \r\n",
        "#                    epochs = 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNaxcn8HC49D"
      },
      "source": [
        "cukubi = ['my name is [MASK]',\r\n",
        "          'I live in london']\r\n",
        "\r\n",
        "vectorize_layer = TextVectorization(\r\n",
        "        max_tokens=100,\r\n",
        "        output_mode=\"int\",\r\n",
        "        # standardize=custom_standardization,\r\n",
        "        output_sequence_length=10,\r\n",
        "        \r\n",
        "    )\r\n",
        "sonc = vectorize_layer.adapt(cukubi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USs96A4lIL_f"
      },
      "source": [
        "vectorize_layer.get_vocabulary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2wgImRddC6k"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAB0auYbkBL_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jU_8aAyEuzVw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXQVRGMqLoVX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qN-jvQNIQE6c"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkn2S9-YyacX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}