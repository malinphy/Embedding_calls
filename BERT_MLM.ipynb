{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_MLM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPPguNqitW/0+PbmK6vdH2Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/malinphy/Embedding_calls/blob/main/BERT_MLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohty4PVDUm8m"
      },
      "source": [
        "import numpy as np \r\n",
        "import pandas as pd\r\n",
        "import random \r\n",
        "import re\r\n",
        "import tensorflow as tf \r\n",
        "from tensorflow import keras \r\n",
        "from tensorflow.keras import layers\r\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
        "\r\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\r\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
        "\r\n",
        "\r\n",
        "from tensorflow.keras.layers import Dense, Embedding, Flatten ,MultiHeadAttention,Dropout\r\n",
        "from tensorflow.keras import Sequential\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOnaKXwrXfZ3",
        "outputId": "f5ec553f-7180-4371-8531-72f864c334b0"
      },
      "source": [
        "small_sentence = [\r\n",
        "                  'my is [MASK]',\r\n",
        "                  # 'I live in london'\r\n",
        "                  ]\r\n",
        "small_tokenizer = Tokenizer()\r\n",
        "small_tokenizer.fit_on_texts(small_sentence)\r\n",
        "small_tokenizer.word_index"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'is': 2, 'mask': 3, 'my': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYyI72eIUn9I"
      },
      "source": [
        "## 1) Sentences will be tokenized using the TextVectorization \r\n",
        "## 2) 12.5 % of the tokens will be replaced with the [MASK] token. Substituded tokens will be \r\n",
        "## chosen randomly. To select the random tokens, random sampling will be chosen \r\n",
        "\r\n",
        "## 3) segment id will be determined at the second step\r\n",
        "## 4) positional embeddings will be directly taken from the article"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T95t9xM_UpXD"
      },
      "source": [
        "max_length =  128  ## 256\r\n",
        "corpus_length = 128   ## 30000\r\n",
        "embedding_dimension = 128   ## 128\r\n",
        "vocab_size = 128  ## 30000\r\n",
        "number_heads = 8\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZXhaqbCUtkC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1d7f4b1-62e0-45ec-8c9e-952459032092"
      },
      "source": [
        "trial_sentences = [\r\n",
        "    'my name is john',\r\n",
        "    'london is the capital is the england',\r\n",
        "    'weather is cold at the northern hemisphere of the world',\r\n",
        "    'today, i will go the school'\r\n",
        "]\r\n",
        "\r\n",
        "\r\n",
        "# prediction_sentence = 'my name is [MASK]'\r\n",
        "# prediction_tokens = tokenizing_procedure(prediction_sentence, tf_tokenizer)\r\n",
        "\r\n",
        "\r\n",
        "longest_sentence = []\r\n",
        "for i in trial_sentences:\r\n",
        "    longest_sentence.append(len(i.split()))\r\n",
        "    \r\n",
        "print(np.max(longest_sentence))\r\n",
        "\r\n",
        "longest_sentence = np.max(longest_sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZt9TQ1lQE20"
      },
      "source": [
        "tf_tokenizer= Tokenizer()\r\n",
        "\r\n",
        "def tokenizing_procedure(text,tokenizer):\r\n",
        "  # this function works as I expected\r\n",
        "  tf_tokenizer = tokenizer\r\n",
        "  tf_tokens = tf_tokenizer.fit_on_texts(text)\r\n",
        "\r\n",
        "  corpus = tf_tokenizer.word_index\r\n",
        "  sequences = tf_tokenizer.texts_to_sequences(text)\r\n",
        "  padded_sequences = pad_sequences(sequences)\r\n",
        "  corpus['MASK'] = len(corpus)+1\r\n",
        "\r\n",
        "  return padded_sequences, corpus\r\n",
        "\r\n",
        "\r\n",
        "padded_sequences, corpus = tokenizing_procedure(trial_sentences, tf_tokenizer)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def masking(corpus, padded_sequences,tokenizer):\r\n",
        "  masked_corpus = corpus.copy()\r\n",
        "  masked_corpus['[MASK]'] = len(corpus)+1\r\n",
        "  corpus_tokens = []   #### TOKEN SET WITHOUT THE SPECIAL TOKENS\r\n",
        "  for i,j in enumerate(masked_corpus):\r\n",
        "      corpus_tokens.append(j)\r\n",
        "\r\n",
        "  subs_tokens=random.sample(corpus_tokens,int(len(corpus_tokens)*0.12))\r\n",
        "  subs_tokens\r\n",
        "\r\n",
        "  subs_tokens_ids = [ ]  ## ID OF THE SUBS TOKENS\r\n",
        "\r\n",
        "  for i in subs_tokens:\r\n",
        "      subs_tokens_ids.append(tf_tokenizer.word_index[i])\r\n",
        "\r\n",
        "  mask_token_id = 20\r\n",
        "  masked_token_id = len(corpus)+1\r\n",
        "\r\n",
        "  for i in range(len(corpus_tokens)):\r\n",
        "        if i == (subs_tokens_ids[0]-1   ):\r\n",
        "            corpus_tokens[i] = '[MASK]'\r\n",
        "    \r\n",
        "        if i == (subs_tokens_ids[1]-1):\r\n",
        "            corpus_tokens[i] = '[MASK]'\r\n",
        "\r\n",
        "  for i in range(padded_sequences.shape[0]):\r\n",
        "        for j in range(padded_sequences.shape[1]):\r\n",
        "          for k in range(len(subs_tokens_ids)):\r\n",
        "            if padded_sequences[i][j] == (subs_tokens_ids[k]):\r\n",
        "\r\n",
        "                padded_sequences[i][j] = masked_token_id \r\n",
        "            if padded_sequences[i][j] == (subs_tokens_ids[k]):\r\n",
        "                \r\n",
        "                padded_sequences[i][j] = masked_token_id \r\n",
        "\r\n",
        "\r\n",
        "  return padded_sequences, masked_token_id\r\n",
        "        \r\n",
        "masked_padded_sequence, masked_token_id = masking(corpus, padded_sequences, tf_tokenizer)\r\n",
        "masked_padded_sequence\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def segment_tokenizer(masked_padded_sequence):\r\n",
        "  segment_ids = masked_padded_sequence.copy()\r\n",
        "  for i in range(segment_ids.shape[0]):\r\n",
        "    for j in range(segment_ids.shape[1]):\r\n",
        "      if segment_ids[i][j] != 0:\r\n",
        "        segment_ids[i][j] = 1\r\n",
        "\r\n",
        "  return segment_ids\r\n",
        "\r\n",
        "segment_ids = segment_tokenizer(masked_padded_sequence)\r\n",
        "# def masked_text_maker(masked_padded_sequences, masked_token_id):\r\n",
        "\r\n",
        "#   for i in range(masked_padded_sequences.shape[0]):\r\n",
        "#         for j in range(masked_padded_sequences.shape[1]):\r\n",
        "#             if masked_padded_sequences[i][j] != 0:\r\n",
        "#                 x = (masked_padded_sequences[i][j]  )\r\n",
        "#                 maskeli.append(\"\".join(extended_tokens[x-1]))\r\n",
        "                \r\n",
        "                \r\n",
        "#   maskeli_2 = []\r\n",
        "#   for i in (masked_padded_sequences):\r\n",
        "#       maskeli_2.append('\\n')\r\n",
        "#       for j in range(len(i)):\r\n",
        "        \r\n",
        "#           if (i[j] !=0):\r\n",
        "# #            print(i[j])\r\n",
        "#               maskeli_2.append(\"\".join(extended_tokens[i[j] -1]))\r\n",
        "    \r\n",
        "    \r\n",
        "#   y= \" \".join(maskeli_2)\r\n",
        "\r\n",
        "#   y = y.split('\\n')\r\n",
        "#   y = np.delete(y,0,0)\r\n",
        "  \r\n",
        "#   return y\r\n",
        "\r\n",
        "# masked_text_maker(masked_padded_sequence, masked_token_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6b3sGLKzfX6",
        "outputId": "cd5cad48-b369-4240-9be3-c12f8c4314d9"
      },
      "source": [
        "total_tokens = segment_ids + masked_padded_sequence\r\n",
        "\r\n",
        "total_tokens.shape\r\n",
        "padded_sequences"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  0,  0,  0,  0,  0,  3,  4, 22,  5],\n",
              "       [ 0,  0,  0,  6, 22,  1,  7, 22,  1,  8],\n",
              "       [ 9, 22, 10, 11,  1, 12, 13, 14,  1, 15],\n",
              "       [ 0,  0,  0,  0, 16, 17, 22, 19,  1, 20]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05Hg842CUvEZ"
      },
      "source": [
        "# def masking_procedure (text,tok):\r\n",
        "    \r\n",
        "#     # tf_tokenizer = Tokenizer()\r\n",
        "#     tf_tokenizer = tok\r\n",
        "#     tf_tokens=tf_tokenizer.fit_on_texts(text)\r\n",
        "\r\n",
        "#     corpus = tf_tokenizer.word_index  ### TOKEN SET WITH TOKEN IDS WITHOUT THE SPECIAL TOKENS \r\n",
        "#     extended_corpus = corpus.copy()   ### TOKEN SET WITH SPECIAL TOKENS\r\n",
        "\r\n",
        "#     extended_corpus['MASK'] = len(corpus)+1\r\n",
        "\r\n",
        "#     corpus_tokens = []   #### TOKEN SET WITHOUT THE SPECIAL TOKENS\r\n",
        "#     for i,j in enumerate(corpus):\r\n",
        "#         corpus_tokens.append(j)\r\n",
        "# #         print(i,j)\r\n",
        "\r\n",
        "        \r\n",
        "#     extended_tokens = []\r\n",
        "#     for i,j in  enumerate(extended_corpus):\r\n",
        "#     #     print(i,j)\r\n",
        "#         extended_tokens.append(j)\r\n",
        "#     # print('maskelenmis_corpus')\r\n",
        "#     # print('-------')\r\n",
        "#     subs_tokens=random.sample(corpus_tokens,int(len(corpus_tokens)*0.12)) ## RANDOMLY SELECTED TOKENS TO BE MASKED\r\n",
        "# #     print(subs_tokens)\r\n",
        "\r\n",
        "#     sequences = tf_tokenizer.texts_to_sequences(text)\r\n",
        "#     padded_sequences= pad_sequences(sequences)\r\n",
        "#     padded_sequences\r\n",
        "\r\n",
        "#     substitute_padded_sequences= padded_sequences.copy()\r\n",
        "#     actual_padded_sequences = padded_sequences.copy()\r\n",
        "\r\n",
        "#     subs_tokens_ids = [ ]  ## ID OF THE SUBS TOKENS\r\n",
        "\r\n",
        "#     for i in subs_tokens:\r\n",
        "#         subs_tokens_ids.append(tf_tokenizer.word_index[i])\r\n",
        "    \r\n",
        "    \r\n",
        "#     mask_token_id = 20\r\n",
        "# #     print(subs_tokens_ids)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "#     for i in range(len(extended_tokens)):\r\n",
        "#         if i == (subs_tokens_ids[0]-1   ):\r\n",
        "#             extended_tokens[i] = '[MASK]'\r\n",
        "    \r\n",
        "#         if i == (subs_tokens_ids[1]-1):\r\n",
        "#             extended_tokens[i] = '[MASK]'\r\n",
        "        \r\n",
        "        \r\n",
        "        \r\n",
        "#     maskeli = []\r\n",
        "#     for i in range(substitute_padded_sequences.shape[0]):\r\n",
        "#         for j in range(substitute_padded_sequences.shape[1]):\r\n",
        "#             if substitute_padded_sequences[i][j] != 0:\r\n",
        "#                 x = (substitute_padded_sequences[i][j]  )\r\n",
        "#                 maskeli.append(\"\".join(extended_tokens[x-1]))\r\n",
        "                \r\n",
        "                \r\n",
        "#     maskeli_2 = []\r\n",
        "#     for i in (substitute_padded_sequences):\r\n",
        "#         maskeli_2.append('\\n')\r\n",
        "#         for j in range(len(i)):\r\n",
        "        \r\n",
        "#             if (i[j] !=0):\r\n",
        "# #             print(i[j])\r\n",
        "#                 maskeli_2.append(\"\".join(extended_tokens[i[j] -1]))\r\n",
        "    \r\n",
        "    \r\n",
        "#     y= \" \".join(maskeli_2)\r\n",
        "\r\n",
        "#     y = y.split('\\n')\r\n",
        "#     y = np.delete(y,0,0)\r\n",
        "#     y\r\n",
        "     \r\n",
        "#     segment_ids = substitute_padded_sequences.copy()\r\n",
        "\r\n",
        "#     for i in range(segment_ids.shape[0]):\r\n",
        "#         for j in range(segment_ids.shape[1]):\r\n",
        "#             if (segment_ids[i][j] != 0):\r\n",
        "#                 segment_ids[i][j] = 1\r\n",
        "#     segment_ids = np.array(segment_ids)\r\n",
        "    \r\n",
        "#     return (y , extended_tokens, segment_ids,substitute_padded_sequences,actual_padded_sequences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mteujAxNQsp"
      },
      "source": [
        "# tf_tokenizer = Tokenizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NtDnwk6Uwei"
      },
      "source": [
        "# masked_text, masked_tokens,segment_ids , masked_sequence, original_sequences = masking_procedure(trial_sentences,tf_tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWJiPeEbwwLP"
      },
      "source": [
        "# masked_sequence,original_sequences,masked_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51p_DKXsUxvm"
      },
      "source": [
        "MAX_LEN = 256\r\n",
        "EMBED_DIM = 128\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def get_pos_encoding_matrix(max_len, d_emb):\r\n",
        "    pos_enc = np.array(\r\n",
        "        [\r\n",
        "            [pos / np.power(10000, 2 * (j // 2) / d_emb) for j in range(d_emb)]\r\n",
        "            if pos != 0\r\n",
        "            else np.zeros(d_emb)\r\n",
        "            for pos in range(max_len)\r\n",
        "        ]\r\n",
        "    )\r\n",
        "    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2])  # dim 2i\r\n",
        "    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2])  # dim 2i+1\r\n",
        "    return pos_enc\r\n",
        "\r\n",
        "\r\n",
        "positional_embeddings = get_pos_encoding_matrix(MAX_LEN, EMBED_DIM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKuES0kuUz-T"
      },
      "source": [
        "# pos_enc = get_pos_encoding_matrix(MAX_LEN, EMBED_DIM)\r\n",
        "# # pos_enc + segment_ids\r\n",
        "\r\n",
        "# total_encoding = masked_sequence+segment_ids # +pos_enc  (bi sekilde positional encodingi sokmam gerek )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEuqxovDCRpY"
      },
      "source": [
        "# total_encoding.shape\r\n",
        "# intro_shape = total_encoding.shape\r\n",
        "# masked_sequence = masked_sequence.astype(dtype='float32')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kP_jJ8FiPoVz"
      },
      "source": [
        "# def encoder_structure (masked_sequence):\r\n",
        "#   emb_layer = layers.Embedding(30000, EMBED_DIM,input_length=max_length)(masked_sequence)\r\n",
        "#   att_layer = layers.MultiHeadAttention(num_heads=8,key_dim=EMBED_DIM)(emb_layer,emb_layer,emb_layer)\r\n",
        "#   drop_layer = layers.Dropout(0.1)(att_layer)\r\n",
        "#   norm_layer = layers.LayerNormalization(epsilon = 1e-6)(emb_layer + drop_layer)\r\n",
        "#   ff1_layer = layers.Dense(EMBED_DIM , activation='relu')(norm_layer)\r\n",
        "\r\n",
        "\r\n",
        "#   #bu kisimda bir noktada activasyon fonksiyonu olmadan sadece embedding layeri kadar neurona sahip\r\n",
        "#   #baska bir layer var onu da eklemem gerebilir\r\n",
        "#   #AYRICA FF1_LAYER'IN CIKIS KISMI SHAPELERINE DIKKAT ET\r\n",
        "\r\n",
        "#   return (ff1_layer)\r\n",
        "\r\n",
        "\r\n",
        "# decision_model = tf.keras.Sequential([\r\n",
        "                                      \r\n",
        "#                                       tf.keras.layers.Dense(10,activation = 'softmax'),\r\n",
        "#                                       tf.keras.layers.Dense(1)\r\n",
        "# ])\r\n",
        "\r\n",
        "# decision_model.compile(\r\n",
        "#     loss= 'categorical_crossentropy',\r\n",
        "#     optimizer =  'Adam',\r\n",
        "#     metrics = ['accuracy']\r\n",
        "# )\r\n",
        "# decision_input = encoder_structure(masked_sequence)\r\n",
        "# decision_model.fit(decision_input, masked_sequence, epochs=1)\r\n",
        "\r\n",
        "# # BU NOKTADAN SONRA SOFTMAX FONKSIYONU KULLANILARAK OLUSTURULAN MODEL TRAIN EDILECEK \r\n",
        "# # INPUT OLARAK ENCODER_STRUCTURE FONKIYONUN \r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vk-MpH0E6GkJ"
      },
      "source": [
        "# masked_sequence.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rsFrO0ICT0A"
      },
      "source": [
        "# emb_layer = layers.Embedding(30000, EMBED_DIM,input_length=max_length)(masked_sequence)\r\n",
        "# att_layer = layers.MultiHeadAttention(num_heads=8,key_dim=EMBED_DIM)(emb_layer,emb_layer,emb_layer)\r\n",
        "# drop_layer = layers.Dropout(0.1)(att_layer)\r\n",
        "# norm_layer = layers.LayerNormalization(epsilon = 1e-6)(emb_layer + drop_layer)\r\n",
        "# ff1_layer = layers.Dense(EMBED_DIM , activation='relu')(norm_layer)\r\n",
        "\r\n",
        "\r\n",
        "# #### FF NN\r\n",
        "\r\n",
        "# ff_model = tf.keras.Sequential([\r\n",
        "#                                 tf.keras.layers.Dense(EMBED_DIM, activation= 'relu'),\r\n",
        "#                                 tf.keras.layers.Dense(1)\r\n",
        "\r\n",
        "# ])\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# # ff_model.compile(\r\n",
        "# #     loss = 'categorical_crossentropy',\r\n",
        "# #     optimizer = 'Adam',\r\n",
        "# #     metrics = ['accuracy']\r\n",
        "# # )\r\n",
        "\r\n",
        "# # ff_model.fit(norm_layer, masked_sequence, epochs =50)\r\n",
        "# print('norm_layer_shape',norm_layer.shape)\r\n",
        "# print('masked_sequence_shape',masked_sequence.shape)\r\n",
        "# print('encoder_structure_out_shape',ff1_layer.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnHvFldIekIN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcf23543-5843-4115-ff18-44ab29cece1d"
      },
      "source": [
        "total_tokens #  INPUT SEQUENCE \r\n",
        "padded_sequences  # LOGITS\r\n",
        "total_tokens.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5YM9XEJekEV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a811aa9f-2b14-4389-801c-cf4886972809"
      },
      "source": [
        "MAX_LEN = 256\r\n",
        "EMBED_DIM = 128\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "input_layer = keras.Input(shape = (total_tokens.shape[1]))\r\n",
        "emb_layer = layers.Embedding(1000, EMBED_DIM)(input_layer)\r\n",
        "att_layer = layers.MultiHeadAttention(num_heads=8,key_dim=EMBED_DIM)(emb_layer,emb_layer,emb_layer)\r\n",
        "drop_layer = layers.Dropout(0.1)(att_layer)\r\n",
        "norm_layer = layers.LayerNormalization(epsilon = 1e-6)(emb_layer + drop_layer)\r\n",
        "ff1_layer = layers.Dense(EMBED_DIM , activation='relu')(norm_layer)\r\n",
        "output_layer = layers.Dense(1)(ff1_layer)\r\n",
        "small_model = keras.Model(input_layer, output_layer)\r\n",
        "\r\n",
        "#### FF NN\r\n",
        "\r\n",
        "ff_model = tf.keras.Sequential([\r\n",
        "                                tf.keras.layers.Dense(EMBED_DIM, activation= 'relu'),\r\n",
        "                                tf.keras.layers.Dense(1)\r\n",
        "\r\n",
        "])\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "small_model.compile(\r\n",
        "    loss = 'categorical_crossentropy',\r\n",
        "    optimizer = 'Adam',\r\n",
        "    metrics = ['accuracy']\r\n",
        ")\r\n",
        "\r\n",
        "small_model.fit(total_tokens, padded_sequences, epochs=3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "1/1 [==============================] - 1s 1s/step - loss: 263.8961 - accuracy: 0.3250\n",
            "Epoch 2/3\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 165.8162 - accuracy: 0.3250\n",
            "Epoch 3/3\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 148.2724 - accuracy: 0.3250\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f3d591cd8d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeQz5O2-GRkB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07a4bd90-d135-47c1-871c-5b44caafd6f9"
      },
      "source": [
        "\r\n",
        "\r\n",
        "prediction_tokens, ikincil = tokenizing_procedure(prediction_sentence, tf_tokenizer)\r\n",
        "np.argmax(small_model.predict(prediction_tokens))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 10) for input KerasTensor(type_spec=TensorSpec(shape=(None, 10), dtype=tf.float32, name='input_5'), name='input_5', description=\"created by layer 'input_5'\"), but it was called on an input with incompatible shape (None, 1).\n",
            "WARNING:tensorflow:5 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fbdcb52b378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LTuqR2FGRgg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0917e3c-5c34-4e23-8b2f-2b3ff4fca360"
      },
      "source": [
        "prediction_sentence = 'my name is MASK'\r\n",
        "tokenizing_procedure(prediction_sentence,tf_tokenizer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[ 1],\n",
              "        [ 7],\n",
              "        [ 0],\n",
              "        [ 8],\n",
              "        [ 2],\n",
              "        [ 1],\n",
              "        [ 9],\n",
              "        [ 0],\n",
              "        [ 6],\n",
              "        [ 3],\n",
              "        [ 0],\n",
              "        [ 1],\n",
              "        [ 2],\n",
              "        [ 3],\n",
              "        [10]], dtype=int32),\n",
              " {'MASK': 28,\n",
              "  'a': 2,\n",
              "  'at': 19,\n",
              "  'capital': 15,\n",
              "  'cold': 18,\n",
              "  'e': 9,\n",
              "  'england': 16,\n",
              "  'go': 26,\n",
              "  'hemisphere': 21,\n",
              "  'i': 6,\n",
              "  'is': 5,\n",
              "  'john': 13,\n",
              "  'k': 10,\n",
              "  'london': 14,\n",
              "  'm': 1,\n",
              "  'my': 11,\n",
              "  'n': 8,\n",
              "  'name': 12,\n",
              "  'northern': 20,\n",
              "  'of': 22,\n",
              "  's': 3,\n",
              "  'school': 27,\n",
              "  'the': 4,\n",
              "  'today': 24,\n",
              "  'weather': 17,\n",
              "  'will': 25,\n",
              "  'world': 23,\n",
              "  'y': 7})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVgpo3toCVaC"
      },
      "source": [
        "# input_layer = layers.Input(shape = (10,))\r\n",
        "# l1_layer = layers.Embedding(corpus_length, 10, input_length =10)(input_layer)\r\n",
        "# # l1_layer = layers.Flatten()(l1_layer)\r\n",
        "# out_layer  = layers.Dense(10, activation= 'softmax')(l1_layer)\r\n",
        "# tf_trial_model  = keras.Model(inputs= input_layer, outputs = out_layer)\r\n",
        "\r\n",
        "# tf_trial_model.compile(\r\n",
        "#     loss = 'categorical_crossentropy',\r\n",
        "#     optimizer = 'Adam',\r\n",
        "#     metrics = ['accuracy']\r\n",
        "# )\r\n",
        "\r\n",
        "# tf_trial_model.fit(\r\n",
        "#                   #  np.transpose(masked_sequence),\r\n",
        "#                    masked_sequence, \r\n",
        "#                    original_sequences, \r\n",
        "#                    epochs = 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNaxcn8HC49D"
      },
      "source": [
        "cukubi = ['my name is [MASK]',\r\n",
        "          'I live in london']\r\n",
        "\r\n",
        "vectorize_layer = TextVectorization(\r\n",
        "        max_tokens=100,\r\n",
        "        output_mode=\"int\",\r\n",
        "        # standardize=custom_standardization,\r\n",
        "        output_sequence_length=10,\r\n",
        "        \r\n",
        "    )\r\n",
        "sonc = vectorize_layer.adapt(cukubi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USs96A4lIL_f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9cea4d0-620a-4adc-8d78-a838cd733234"
      },
      "source": [
        "vectorize_layer.get_vocabulary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '[UNK]', 'name', 'my', 'mask', 'london', 'live', 'is', 'in', 'i']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2wgImRddC6k"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAB0auYbkBL_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jU_8aAyEuzVw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXQVRGMqLoVX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qN-jvQNIQE6c"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkn2S9-YyacX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}