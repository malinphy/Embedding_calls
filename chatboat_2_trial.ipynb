{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chatboat_2_trial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMAkvtkVHvCYcuLf0k9GKJk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/malinphy/Embedding_calls/blob/main/chatboat_2_trial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6a-XxKdHMXqg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cef7d7b-b8e7-42ad-9bf1-685018dd410a"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf \n",
        "from tensorflow import keras \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import re\n",
        "import os\n",
        "import sys\n",
        "!pip install -q tensorflow_text\n",
        "\n",
        "import tensorflow_text as txt\n",
        "import tensorflow_datasets as tfds\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |                                | 10kB 26.0MB/s eta 0:00:01\r\u001b[K     |▏                               | 20kB 33.7MB/s eta 0:00:01\r\u001b[K     |▎                               | 30kB 24.2MB/s eta 0:00:01\r\u001b[K     |▍                               | 40kB 27.6MB/s eta 0:00:01\r\u001b[K     |▌                               | 51kB 25.9MB/s eta 0:00:01\r\u001b[K     |▋                               | 61kB 28.6MB/s eta 0:00:01\r\u001b[K     |▊                               | 71kB 18.9MB/s eta 0:00:01\r\u001b[K     |▉                               | 81kB 20.3MB/s eta 0:00:01\r\u001b[K     |▉                               | 92kB 19.2MB/s eta 0:00:01\r\u001b[K     |█                               | 102kB 19.0MB/s eta 0:00:01\r\u001b[K     |█                               | 112kB 19.0MB/s eta 0:00:01\r\u001b[K     |█▏                              | 122kB 19.0MB/s eta 0:00:01\r\u001b[K     |█▎                              | 133kB 19.0MB/s eta 0:00:01\r\u001b[K     |█▍                              | 143kB 19.0MB/s eta 0:00:01\r\u001b[K     |█▌                              | 153kB 19.0MB/s eta 0:00:01\r\u001b[K     |█▋                              | 163kB 19.0MB/s eta 0:00:01\r\u001b[K     |█▋                              | 174kB 19.0MB/s eta 0:00:01\r\u001b[K     |█▊                              | 184kB 19.0MB/s eta 0:00:01\r\u001b[K     |█▉                              | 194kB 19.0MB/s eta 0:00:01\r\u001b[K     |██                              | 204kB 19.0MB/s eta 0:00:01\r\u001b[K     |██                              | 215kB 19.0MB/s eta 0:00:01\r\u001b[K     |██▏                             | 225kB 19.0MB/s eta 0:00:01\r\u001b[K     |██▎                             | 235kB 19.0MB/s eta 0:00:01\r\u001b[K     |██▍                             | 245kB 19.0MB/s eta 0:00:01\r\u001b[K     |██▍                             | 256kB 19.0MB/s eta 0:00:01\r\u001b[K     |██▌                             | 266kB 19.0MB/s eta 0:00:01\r\u001b[K     |██▋                             | 276kB 19.0MB/s eta 0:00:01\r\u001b[K     |██▊                             | 286kB 19.0MB/s eta 0:00:01\r\u001b[K     |██▉                             | 296kB 19.0MB/s eta 0:00:01\r\u001b[K     |███                             | 307kB 19.0MB/s eta 0:00:01\r\u001b[K     |███                             | 317kB 19.0MB/s eta 0:00:01\r\u001b[K     |███▏                            | 327kB 19.0MB/s eta 0:00:01\r\u001b[K     |███▏                            | 337kB 19.0MB/s eta 0:00:01\r\u001b[K     |███▎                            | 348kB 19.0MB/s eta 0:00:01\r\u001b[K     |███▍                            | 358kB 19.0MB/s eta 0:00:01\r\u001b[K     |███▌                            | 368kB 19.0MB/s eta 0:00:01\r\u001b[K     |███▋                            | 378kB 19.0MB/s eta 0:00:01\r\u001b[K     |███▊                            | 389kB 19.0MB/s eta 0:00:01\r\u001b[K     |███▉                            | 399kB 19.0MB/s eta 0:00:01\r\u001b[K     |████                            | 409kB 19.0MB/s eta 0:00:01\r\u001b[K     |████                            | 419kB 19.0MB/s eta 0:00:01\r\u001b[K     |████                            | 430kB 19.0MB/s eta 0:00:01\r\u001b[K     |████▏                           | 440kB 19.0MB/s eta 0:00:01\r\u001b[K     |████▎                           | 450kB 19.0MB/s eta 0:00:01\r\u001b[K     |████▍                           | 460kB 19.0MB/s eta 0:00:01\r\u001b[K     |████▌                           | 471kB 19.0MB/s eta 0:00:01\r\u001b[K     |████▋                           | 481kB 19.0MB/s eta 0:00:01\r\u001b[K     |████▊                           | 491kB 19.0MB/s eta 0:00:01\r\u001b[K     |████▊                           | 501kB 19.0MB/s eta 0:00:01\r\u001b[K     |████▉                           | 512kB 19.0MB/s eta 0:00:01\r\u001b[K     |█████                           | 522kB 19.0MB/s eta 0:00:01\r\u001b[K     |█████                           | 532kB 19.0MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 542kB 19.0MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 552kB 19.0MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 563kB 19.0MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 573kB 19.0MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 583kB 19.0MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 593kB 19.0MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 604kB 19.0MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 614kB 19.0MB/s eta 0:00:01\r\u001b[K     |██████                          | 624kB 19.0MB/s eta 0:00:01\r\u001b[K     |██████                          | 634kB 19.0MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 645kB 19.0MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 655kB 19.0MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 665kB 19.0MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 675kB 19.0MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 686kB 19.0MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 696kB 19.0MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 706kB 19.0MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 716kB 19.0MB/s eta 0:00:01\r\u001b[K     |███████                         | 727kB 19.0MB/s eta 0:00:01\r\u001b[K     |███████                         | 737kB 19.0MB/s eta 0:00:01\r\u001b[K     |███████                         | 747kB 19.0MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 757kB 19.0MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 768kB 19.0MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 778kB 19.0MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 788kB 19.0MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 798kB 19.0MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 808kB 19.0MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 819kB 19.0MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 829kB 19.0MB/s eta 0:00:01\r\u001b[K     |████████                        | 839kB 19.0MB/s eta 0:00:01\r\u001b[K     |████████                        | 849kB 19.0MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 860kB 19.0MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 870kB 19.0MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 880kB 19.0MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 890kB 19.0MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 901kB 19.0MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 911kB 19.0MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 921kB 19.0MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 931kB 19.0MB/s eta 0:00:01\r\u001b[K     |█████████                       | 942kB 19.0MB/s eta 0:00:01\r\u001b[K     |█████████                       | 952kB 19.0MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 962kB 19.0MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 972kB 19.0MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 983kB 19.0MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 993kB 19.0MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 1.0MB 19.0MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 1.0MB 19.0MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 1.0MB 19.0MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 1.0MB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████                      | 1.0MB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████                      | 1.1MB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 1.1MB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 1.1MB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 1.1MB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 1.1MB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 1.1MB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 1.1MB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 1.1MB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 1.1MB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████                     | 1.1MB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████                     | 1.2MB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████                     | 1.2MB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 1.2MB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 1.2MB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 1.2MB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 1.2MB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 1.2MB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 1.2MB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 1.2MB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 1.2MB 19.0MB/s eta 0:00:01\r\u001b[K     |████████████                    | 1.3MB 19.0MB/s eta 0:00:01\r\u001b[K     |████████████                    | 1.3MB 19.0MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 1.3MB 19.0MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 1.3MB 19.0MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 1.3MB 19.0MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 1.3MB 19.0MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 1.3MB 19.0MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 1.3MB 19.0MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 1.3MB 19.0MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 1.4MB 19.0MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 1.4MB 19.0MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 1.4MB 19.0MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 1.4MB 19.0MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 1.4MB 19.0MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 1.4MB 19.0MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 1.4MB 19.0MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 1.4MB 19.0MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 1.4MB 19.0MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 1.4MB 19.0MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 1.5MB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 1.5MB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 1.5MB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 1.5MB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 1.5MB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 1.5MB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 1.5MB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 1.5MB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 1.5MB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 1.5MB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 1.6MB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 1.6MB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 1.6MB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 1.6MB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 1.6MB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 1.6MB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 1.6MB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 1.6MB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 1.6MB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 1.6MB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 1.7MB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 1.7MB 19.0MB/s eta 0:00:01\r\u001b[K     |████████████████                | 1.7MB 19.0MB/s eta 0:00:01\r\u001b[K     |████████████████                | 1.7MB 19.0MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 1.7MB 19.0MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 1.7MB 19.0MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 1.7MB 19.0MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 1.7MB 19.0MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 1.7MB 19.0MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 1.8MB 19.0MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 1.8MB 19.0MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 1.8MB 19.0MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.8MB 19.0MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.8MB 19.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 1.8MB 19.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 1.8MB 19.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 1.8MB 19.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 1.8MB 19.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 1.8MB 19.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 1.9MB 19.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 1.9MB 19.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 1.9MB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.9MB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.9MB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.9MB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 1.9MB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 1.9MB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 1.9MB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 1.9MB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 2.0MB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 2.0MB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 2.0MB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 2.0MB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 2.0MB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 2.0MB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 2.0MB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 2.0MB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 2.0MB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 2.0MB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 2.1MB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 2.1MB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 2.1MB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 2.1MB 19.0MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 2.1MB 19.0MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 2.1MB 19.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 2.1MB 19.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 2.1MB 19.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 2.1MB 19.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 2.2MB 19.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 2.2MB 19.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 2.2MB 19.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 2.2MB 19.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 2.2MB 19.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 2.2MB 19.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 2.2MB 19.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 2.2MB 19.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 2.2MB 19.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 2.2MB 19.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 2.3MB 19.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 2.3MB 19.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 2.3MB 19.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 2.3MB 19.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 2.3MB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 2.3MB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 2.3MB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 2.3MB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 2.3MB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 2.3MB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 2.4MB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 2.4MB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 2.4MB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 2.4MB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 2.4MB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 2.4MB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 2.4MB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 2.4MB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 2.4MB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 2.4MB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 2.5MB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 2.5MB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 2.5MB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 2.5MB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 2.5MB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 2.5MB 19.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 2.5MB 19.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 2.5MB 19.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 2.5MB 19.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 2.5MB 19.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 2.6MB 19.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 2.6MB 19.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 2.6MB 19.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 2.6MB 19.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 2.6MB 19.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 2.6MB 19.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 2.6MB 19.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 2.6MB 19.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 2.6MB 19.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 2.7MB 19.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 2.7MB 19.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 2.7MB 19.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 2.7MB 19.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 2.7MB 19.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 2.7MB 19.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 2.7MB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 2.7MB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 2.7MB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 2.7MB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 2.8MB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 2.8MB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 2.8MB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 2.8MB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 2.8MB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 2.8MB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 2.8MB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 2.8MB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 2.8MB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 2.8MB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 2.9MB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 2.9MB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 2.9MB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 2.9MB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 2.9MB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 2.9MB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 2.9MB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 2.9MB 19.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 2.9MB 19.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 2.9MB 19.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 3.0MB 19.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 3.0MB 19.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 3.0MB 19.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 3.0MB 19.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 3.0MB 19.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 3.0MB 19.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 3.0MB 19.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 3.0MB 19.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 3.0MB 19.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 3.1MB 19.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 3.1MB 19.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 3.1MB 19.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 3.1MB 19.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 3.1MB 19.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 3.1MB 19.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 3.1MB 19.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 3.1MB 19.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 3.1MB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 3.1MB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 3.2MB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 3.2MB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 3.2MB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 3.2MB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 3.2MB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 3.2MB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 3.2MB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 3.2MB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 3.2MB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 3.2MB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 3.3MB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 3.3MB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 3.3MB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 3.3MB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 3.3MB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 3.3MB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 3.3MB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 3.3MB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 3.3MB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 3.3MB 19.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 3.4MB 19.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 3.4MB 19.0MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHBuqfnlN2Wc"
      },
      "source": [
        "# Maximum sentence length\n",
        "MAX_LENGTH = 40\n",
        "\n",
        "# Maximum number of samples to preprocess\n",
        "MAX_SAMPLES = 50000\n",
        "\n",
        "# For tf.data.Dataset\n",
        "# BATCH_SIZE = 64 * strategy.num_replicas_in_sync\n",
        "BATCH_SIZE = 16\n",
        "BUFFER_SIZE = 20000\n",
        "\n",
        "# For Transformer\n",
        "NUM_LAYERS = 2\n",
        "D_MODEL = 256\n",
        "NUM_HEADS = 8\n",
        "UNITS = 512\n",
        "DROPOUT = 0.1\n",
        "\n",
        "EPOCHS = 40"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3w0rQI4DNsan"
      },
      "source": [
        "def preprocess_sentence(sentence):\n",
        "  sentence = sentence.lower().strip()\n",
        "  # creating a space between a word and the punctuation following it\n",
        "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
        "  sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
        "  # removing contractions\n",
        "  sentence = re.sub(r\"i'm\", \"i am\", sentence)\n",
        "  sentence = re.sub(r\"he's\", \"he is\", sentence)\n",
        "  sentence = re.sub(r\"she's\", \"she is\", sentence)\n",
        "  sentence = re.sub(r\"it's\", \"it is\", sentence)\n",
        "  sentence = re.sub(r\"that's\", \"that is\", sentence)\n",
        "  sentence = re.sub(r\"what's\", \"that is\", sentence)\n",
        "  sentence = re.sub(r\"where's\", \"where is\", sentence)\n",
        "  sentence = re.sub(r\"how's\", \"how is\", sentence)\n",
        "  sentence = re.sub(r\"\\'ll\", \" will\", sentence)\n",
        "  sentence = re.sub(r\"\\'ve\", \" have\", sentence)\n",
        "  sentence = re.sub(r\"\\'re\", \" are\", sentence)\n",
        "  sentence = re.sub(r\"\\'d\", \" would\", sentence)\n",
        "  sentence = re.sub(r\"\\'re\", \" are\", sentence)\n",
        "  sentence = re.sub(r\"won't\", \"will not\", sentence)\n",
        "  sentence = re.sub(r\"can't\", \"cannot\", sentence)\n",
        "  sentence = re.sub(r\"n't\", \" not\", sentence)\n",
        "  sentence = re.sub(r\"n'\", \"ng\", sentence)\n",
        "  sentence = re.sub(r\"'bout\", \"about\", sentence)\n",
        "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "  sentence = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", sentence)\n",
        "  sentence = sentence.strip()\n",
        "  return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FD4wp17iMxBu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea42b817-7f20-49a6-f57d-876208e26e16"
      },
      "source": [
        "path_to_zip = tf.keras.utils.get_file(\n",
        "    'cornell_movie_dialogs.zip',\n",
        "    origin=\n",
        "    'http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip',\n",
        "    extract=True)\n",
        "\n",
        "path_to_dataset = os.path.join(\n",
        "    os.path.dirname(path_to_zip), \"cornell movie-dialogs corpus\")\n",
        "\n",
        "path_to_movie_lines = os.path.join(path_to_dataset, 'movie_lines.txt')\n",
        "path_to_movie_conversations = os.path.join(path_to_dataset,\n",
        "                                           'movie_conversations.txt')\n",
        "\n",
        "\n",
        "def load_conversations():\n",
        "  # dictionary of line id to text\n",
        "  id2line = {}\n",
        "  with open(path_to_movie_lines, errors='ignore') as file:\n",
        "    lines = file.readlines()\n",
        "  for line in lines:\n",
        "    parts = line.replace('\\n', '').split(' +++$+++ ')\n",
        "    id2line[parts[0]] = parts[4]\n",
        "\n",
        "  inputs, outputs = [], []\n",
        "  with open(path_to_movie_conversations, 'r') as file:\n",
        "    lines = file.readlines()\n",
        "  for line in lines:\n",
        "    parts = line.replace('\\n', '').split(' +++$+++ ')\n",
        "    # get conversation in a list of line ID\n",
        "    conversation = [line[1:-1] for line in parts[3][1:-1].split(', ')]\n",
        "    for i in range(len(conversation) - 1):\n",
        "      inputs.append(preprocess_sentence(id2line[conversation[i]]))\n",
        "      outputs.append(preprocess_sentence(id2line[conversation[i + 1]]))\n",
        "      if len(inputs) >= MAX_SAMPLES:\n",
        "        return inputs, outputs\n",
        "  return inputs, outputs\n",
        "\n",
        "\n",
        "questions, answers = load_conversations()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\n",
            "9920512/9916637 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAbnn6Q2NiQF"
      },
      "source": [
        "\n",
        "\n",
        "# Build tokenizer using tfds for both questions and answers\n",
        "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    questions + answers, target_vocab_size=2**13)\n",
        "\n",
        "# Define start and end token to indicate the start and end of a sentence\n",
        "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
        "\n",
        "# Vocabulary size plus start and end token\n",
        "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UB7hCUlOWYl"
      },
      "source": [
        "def tokenize_and_filter(inputs, outputs):\n",
        "  tokenized_inputs, tokenized_outputs = [], []\n",
        "  \n",
        "  for (sentence1, sentence2) in zip(inputs, outputs):\n",
        "    # tokenize sentence\n",
        "    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
        "    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
        "    # check tokenized sentence max length\n",
        "    if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
        "      tokenized_inputs.append(sentence1)\n",
        "      tokenized_outputs.append(sentence2)\n",
        "  \n",
        "  # pad tokenized sentences\n",
        "  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "      tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
        "  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
        "  \n",
        "  return tokenized_inputs, tokenized_outputs\n",
        "\n",
        "\n",
        "questions, answers = tokenize_and_filter(questions, answers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8ZWbfaSOiLT"
      },
      "source": [
        "\n",
        "\n",
        "# # decoder inputs use the previous target as input\n",
        "# # remove START_TOKEN from targets\n",
        "dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\n",
        "        'inputs': questions,\n",
        "        'dec_inputs': answers[:, :-1]\n",
        "    },\n",
        "    {\n",
        "        'outputs': answers[:, 1:]\n",
        "    },\n",
        "))\n",
        "\n",
        "dataset = dataset.cache()\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "# "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeMTGjwXOlmL"
      },
      "source": [
        "# dataset = tf.data.Dataset.from_tensor_slices(\n",
        "#     [tf.ragged.constant(questions),tf.ragged.constant(answers[:, :-1]), \n",
        "#      tf.ragged.constant(answers[:, 1:])]\n",
        "# )\n",
        "\n",
        "# dataset = dataset.cache()\n",
        "# dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "# dataset = dataset.batch(BATCH_SIZE)\n",
        "# dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVUHmiwLUH36"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFcs8-uAUpEc"
      },
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "  return pos * angle_rates"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJatvLV5Uve4"
      },
      "source": [
        "def positional_encoding(position, d_model):\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "\n",
        "  # apply sin to even indices in the array; 2i\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "  # apply cos to odd indices in the array; 2i+1\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjGg-pJOCAwl"
      },
      "source": [
        "def create_padding_mask(seq):\n",
        "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "\n",
        "  # add extra dimensions to add the padding\n",
        "  # to the attention logits.\n",
        "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LulR3GQYCCt6"
      },
      "source": [
        "def create_look_ahead_mask(size):\n",
        "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "  return mask  # (seq_len, seq_len)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_fWuAmxCEID"
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "  \"\"\"Calculate the attention weights.\n",
        "  q, k, v must have matching leading dimensions.\n",
        "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "  The mask has different shapes depending on its type(padding or look ahead)\n",
        "  but it must be broadcastable for addition.\n",
        "\n",
        "  Args:\n",
        "    q: query shape == (..., seq_len_q, depth)\n",
        "    k: key shape == (..., seq_len_k, depth)\n",
        "    v: value shape == (..., seq_len_v, depth_v)\n",
        "    mask: Float tensor with shape broadcastable\n",
        "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "\n",
        "  Returns:\n",
        "    output, attention_weights\n",
        "  \"\"\"\n",
        "\n",
        "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "  # scale matmul_qk\n",
        "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "  # add the mask to the scaled tensor.\n",
        "  if mask is not None:\n",
        "    scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "  # add up to 1.\n",
        "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "  return output, attention_weights\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEubk8kjCFsJ"
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "\n",
        "    assert d_model % self.num_heads == 0\n",
        "\n",
        "    self.depth = d_model // self.num_heads\n",
        "\n",
        "    self.wq = tf.keras.layers.Dense(d_model)\n",
        "    self.wk = tf.keras.layers.Dense(d_model)\n",
        "    self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "  def split_heads(self, x, batch_size):\n",
        "    \"\"\"Split the last dimension into (num_heads, depth).\n",
        "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "    \"\"\"\n",
        "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "  def call(self, v, k, q, mask):\n",
        "    batch_size = tf.shape(q)[0]\n",
        "\n",
        "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "\n",
        "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "        q, k, v, mask)\n",
        "\n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "    concat_attention = tf.reshape(scaled_attention,\n",
        "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "    return output, attention_weights\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1DWtzANCIlf"
      },
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "  ])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkJjOqdaCKi2"
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "\n",
        "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "    attn_output = self.dropout1(attn_output, training=training)\n",
        "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\n",
        "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    return out2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSDpsgEtCM2D"
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding,\n",
        "                                            self.d_model)\n",
        "\n",
        "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate)\n",
        "                       for _ in range(num_layers)]\n",
        "\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "\n",
        "    # adding embedding and position encoding.\n",
        "    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x, training, mask)\n",
        "\n",
        "    return x  # (batch_size, input_seq_len, d_model)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWHio00gCPBz"
      },
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "    self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, enc_output, training,\n",
        "           look_ahead_mask, padding_mask):\n",
        "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
        "    attn1 = self.dropout1(attn1, training=training)\n",
        "    out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "    attn2, attn_weights_block2 = self.mha2(\n",
        "        enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
        "    attn2 = self.dropout2(attn2, training=training)\n",
        "    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
        "    ffn_output = self.dropout3(ffn_output, training=training)\n",
        "    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    return out3, attn_weights_block1, attn_weights_block2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1O9pCgmCQNI"
      },
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate)\n",
        "                       for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, enc_output, training,\n",
        "           look_ahead_mask, padding_mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    attention_weights = {}\n",
        "\n",
        "    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
        "                                             look_ahead_mask, padding_mask)\n",
        "\n",
        "      attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
        "      attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
        "\n",
        "    # x.shape == (batch_size, target_seq_len, d_model)\n",
        "    return x, attention_weights\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4d-_qKVHCR8U"
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "    super(Transformer, self).__init__()\n",
        "\n",
        "    self.tokenizer = Encoder(num_layers, d_model, num_heads, dff,\n",
        "                             input_vocab_size, pe_input, rate)\n",
        "\n",
        "    self.decoder = Decoder(num_layers, d_model, num_heads, dff,\n",
        "                           target_vocab_size, pe_target, rate)\n",
        "\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "  def call(self, inp, tar, training, enc_padding_mask,\n",
        "           look_ahead_mask, dec_padding_mask):\n",
        "\n",
        "    enc_output = self.tokenizer(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
        "\n",
        "    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "    dec_output, attention_weights = self.decoder(\n",
        "        tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "\n",
        "    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "\n",
        "    return final_output, attention_weights\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDM5s2P-CTv1"
      },
      "source": [
        "num_layers = 4\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "dropout_rate = 0.1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vL5y_maYCV6L"
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
        "                                     epsilon=1e-9)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lr5OWV00CWzc"
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
        "\n",
        "\n",
        "def accuracy_function(real, pred):\n",
        "  accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
        "\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  accuracies = tf.math.logical_and(mask, accuracies)\n",
        "\n",
        "  accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
        "  mask = tf.cast(mask, dtype=tf.float32)\n",
        "  return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)\n",
        "\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8UAuLmfCaAB"
      },
      "source": [
        "transformer = Transformer(\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    input_vocab_size=10000  ,#tokenizers.pt.get_vocab_size(),\n",
        "    target_vocab_size=10000, #tokenizers.en.get_vocab_size(),\n",
        "    pe_input=1000,\n",
        "    pe_target=1000,\n",
        "    rate=dropout_rate)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r25FYZC6CgpF"
      },
      "source": [
        "def create_masks(inp, tar):\n",
        "  # Encoder padding mask\n",
        "  enc_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "  # Used in the 2nd attention block in the decoder.\n",
        "  # This padding mask is used to mask the encoder outputs.\n",
        "  dec_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "  # Used in the 1st attention block in the decoder.\n",
        "  # It is used to pad and mask future tokens in the input received by\n",
        "  # the decoder.\n",
        "  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "  dec_target_padding_mask = create_padding_mask(tar)\n",
        "  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "\n",
        "  return enc_padding_mask, combined_mask, dec_padding_mask\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIRs5A76GPOm"
      },
      "source": [
        "checkpoint_path = \"./checkpoints/train\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "# if a checkpoint exists, restore the latest checkpoint.\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "  print('Latest checkpoint restored!!')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NS2WrzpGROc"
      },
      "source": [
        "EPOCHS = 20\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDh9O9ufGadz"
      },
      "source": [
        "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
        "# execution. The function specializes to the precise shape of the argument\n",
        "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
        "# batch sizes (the last batch is smaller), use input_signature to specify\n",
        "# more generic shapes.\n",
        "\n",
        "train_step_signature = [\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "]\n",
        "\n",
        "\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def train_step(inp, tar):\n",
        "  tar_inp = tar[:, :-1]\n",
        "  tar_real = tar[:, 1:]\n",
        "\n",
        "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions, _ = transformer(inp, tar_inp,\n",
        "                                 True,\n",
        "                                 enc_padding_mask,\n",
        "                                 combined_mask,\n",
        "                                 dec_padding_mask)\n",
        "    loss = loss_function(tar_real, predictions)\n",
        "\n",
        "  gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "  train_loss(loss)\n",
        "  train_accuracy(accuracy_function(tar_real, predictions))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "PpfhbioPGbcF",
        "outputId": "b76a5739-d752-4bfb-9535-724022fd4f87"
      },
      "source": [
        "def vector_seq(sequences, dimension=10000):\n",
        "    results = np.zeros(((sequences)))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        results[i, sequence] = 1.\n",
        "    return results\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  # start = time.time()\n",
        "\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "\n",
        "  # inp -> portuguese, tar -> english\n",
        "  for (batch, (inp, tar)) in enumerate(dataset):\n",
        "    # np_list = np.asarray(sentence_list)\n",
        "    inp_list =np.asarray(inp)\n",
        "    inp_tens = vector_seq(inp_list)\n",
        "\n",
        "    tar_list =np.asarray(tar)\n",
        "    tar_tens = vector_seq(tar_list)\n",
        "\n",
        "    train_step(inp_tens, tar_tens)\n",
        "\n",
        "    if batch % 50 == 0:\n",
        "      print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print(f'Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}')\n",
        "\n",
        "  print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "\n",
        "  print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-b28f54eb9b04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# np_list = np.asarray(sentence_list)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0minp_list\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0minp_tens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvector_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mtar_list\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-b28f54eb9b04>\u001b[0m in \u001b[0;36mvector_seq\u001b[0;34m(sequences, dimension)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvector_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimension\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zljASjshHPFI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}